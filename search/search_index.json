{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#microgpt-a-first-principles-course","title":"MicroGPT: A First-Principles Course","text":"<p> Reverse-engineer every single line of a 200-line GPT language model. From 10th-grade math to building your first LLM. No magic, no hand-waving. </p> <p>Start Learning  View the Code </p>"},{"location":"#what-youll-build-a-mental-model-for","title":"What You'll Build a Mental Model For","text":"<pre><code>flowchart LR\n    A[\"\ud83d\udcdd Raw Text\"] --&gt; B[\"\ud83d\udd22 Tokenization\"]\n    B --&gt; C[\"\ud83d\udcd0 Embeddings\"]\n    C --&gt; D[\"\ud83e\udde0 Transformer\"]\n    D --&gt; E[\"\ud83d\udcca Probabilities\"]\n    E --&gt; F[\"\u2728 Generated Text\"]\n\n    style A fill:#1de9b6,stroke:#1de9b6,color:#fff\n    style B fill:#17c9a0,stroke:#17c9a0,color:#fff\n    style C fill:#12a889,stroke:#12a889,color:#fff\n    style D fill:#0d8872,stroke:#0d8872,color:#fff\n    style E fill:#096b5b,stroke:#096b5b,color:#fff\n    style F fill:#1de9b6,stroke:#1de9b6,color:#000</code></pre>"},{"location":"#course-modules","title":"Course Modules","text":""},{"location":"#module-0-the-big-picture","title":"Module 0 \u2014 The Big Picture","text":"<p>What is a language model? A bird's-eye view of the 200 lines and the mental model for how learning machines work.</p>"},{"location":"#module-1-data-tokenization","title":"Module 1 \u2014 Data &amp; Tokenization","text":"<p>How raw text becomes numbers. Character encoding, vocabularies, and the special BOS token.</p>"},{"location":"#module-2-calculus-autograd","title":"Module 2 \u2014 Calculus &amp; Autograd","text":"<p>Derivatives, the chain rule, and how <code>microgpt.py</code> automatically computes gradients with the <code>Value</code> class.</p>"},{"location":"#module-3-the-architecture","title":"Module 3 \u2014 The Architecture","text":"<p>Embeddings, linear layers, softmax, attention, multi-head attention, residual connections, and the full GPT function.</p>"},{"location":"#module-4-training","title":"Module 4 \u2014 Training","text":"<p>Loss functions, backpropagation, gradient descent, the Adam optimizer, and the complete training loop.</p>"},{"location":"#module-5-inference-generation","title":"Module 5 \u2014 Inference &amp; Generation","text":"<p>Using the trained model to generate new text. Temperature, sampling, and the complete picture.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>What you need to know</p> <ul> <li>Math: 10th-grade level \u2014 basic algebra and exponents. There's a Math Refresher if you need it.</li> <li>Programming: Basic Python \u2014 variables, loops, functions, lists.</li> <li>Machine Learning: Zero prior knowledge required.</li> </ul>"},{"location":"#based-on","title":"Based On","text":"<p>This course is built around <code>microgpt.py</code> by Andrej Karpathy \u2014 a complete GPT language model in just 200 lines of pure Python using only the standard library.</p> <p>It implements:</p> <ul> <li>[x] A custom autograd engine (automatic differentiation)</li> <li>[x] A Transformer architecture (attention, MLP, residual connections)</li> <li>[x] A training loop with the Adam optimizer</li> <li>[x] Text generation with temperature-controlled sampling</li> </ul>"},{"location":"00-the-big-picture/00-what-is-a-language-model/","title":"What Is a Language Model?","text":""},{"location":"00-the-big-picture/00-what-is-a-language-model/#the-question-that-started-it-all","title":"The Question That Started It All","text":"<p>Imagine you're playing a game. I show you a sentence with one word missing at the end:</p> <p>\"The cat sat on the ___\"</p> <p>You'd probably guess \"mat\", or maybe \"floor\", or \"couch\". You'd never guess \"quantum\" or \"photosynthesis\". Why? Because you've read enough English to know what words tend to follow other words.</p> <p>The Core Idea</p> <p>A language model is a program that plays this game. Given some text, it predicts what comes next.</p>"},{"location":"00-the-big-picture/00-what-is-a-language-model/#but-how","title":"But How?","text":"<p>Here's the thing \u2014 a computer doesn't \"understand\" English. It doesn't know what a cat is or what sitting means. All it can do is math on numbers.</p> <p>So the entire challenge boils down to:</p> <p>The Fundamental Question</p> <p>How do you turn \"predicting the next word\" into a math problem that a computer can solve?</p> <p>The answer to this question is what the 200 lines of <code>microgpt.py</code> contain.</p>"},{"location":"00-the-big-picture/00-what-is-a-language-model/#the-core-loop","title":"The Core Loop","text":"<p>Every language model \u2014 from the simplest to ChatGPT \u2014 does exactly this:</p> <pre><code>flowchart LR\n    A[\"1. Take some text\\nas input\"] --&gt; B[\"2. Convert it\\nto numbers\"]\n    B --&gt; C[\"3. Do math on\\nthose numbers\"]\n    C --&gt; D[\"4. Get a probability\\nfor every possible\\nnext character\"]\n    D --&gt; E[\"5. Pick one\"]\n    E --&gt; A\n\n    style A fill:#1de9b6,stroke:#0db99a,color:#fff\n    style B fill:#17c9a0,stroke:#4a2db8,color:#fff\n    style C fill:#12a889,stroke:#0a7d68,color:#fff\n    style D fill:#0d8872,stroke:#085c4e,color:#fff\n    style E fill:#1de9b6,stroke:#0db99a,color:#000</code></pre> <p>But wait \u2014 how does the model know which math to do? The math involves multiplying inputs by parameters (thousands of numbers), and those parameters start out random.</p> <p>That's where training comes in:</p> <pre><code>flowchart TD\n    A[\"1. Show the model\\nsome real text\"] --&gt; B[\"2. Let it predict\\nthe next character\"]\n    B --&gt; C[\"3. Measure how\\nWRONG it was\"]\n    C --&gt; D[\"4. Figure out which\\nparameters caused the error\"]\n    D --&gt; E[\"5. Nudge those\\nparameters slightly\"]\n    E --&gt; F[\"6. Repeat\\nthousands of times\"]\n    F --&gt; A\n\n    style A fill:#1de9b6,stroke:#0db99a,color:#fff\n    style B fill:#17c9a0,stroke:#4a2db8,color:#fff\n    style C fill:#64ffda,stroke:#4dd4b0,color:#fff\n    style D fill:#80ffe5,stroke:#5cd4bc,color:#fff\n    style E fill:#2ecc71,stroke:#27ae60,color:#fff\n    style F fill:#1de9b6,stroke:#0db99a,color:#000</code></pre> <p>After enough repetitions, the parameters settle into values that make the model good at predicting.</p>"},{"location":"00-the-big-picture/00-what-is-a-language-model/#character-level-vs-word-level","title":"Character-Level vs. Word-Level","text":"<p>ChatGPT predicts tokens (roughly word-pieces). But <code>microgpt.py</code> predicts individual characters \u2014 one letter at a time.</p> <p>Why characters?</p> <p>Because it's simpler. There are only ~27 characters (a-z plus a few special ones) versus tens of thousands of words. The core algorithm is identical; character-level just makes everything smaller and easier to understand.</p> <p>In our case, the dataset is a list of human names:</p> Training Data (sample)<pre><code>emma\nolivia\nava\nisabella\nsophia\n...\n</code></pre> <p>The model will learn patterns like:</p> <ul> <li>Names often start with certain letters (e, a, s, m...)</li> <li>Certain letter pairs are common (\"th\", \"an\", \"ar\"...)</li> <li>Names tend to be 3-8 characters long</li> <li>Names end at some point (they don't go on forever)</li> </ul>"},{"location":"00-the-big-picture/00-what-is-a-language-model/#what-youll-build-mental-models-for","title":"What You'll Build Mental Models For","text":"<p>By the end of this course, you'll understand:</p> Concept What it means in plain English Tokenization Converting letters to numbers Embeddings Giving each number a richer \"meaning\" Attention Letting the model look at all previous characters to decide the next one Parameters The thousands of numbers that encode what the model has \"learned\" Loss A single number measuring \"how wrong was the prediction?\" Gradient \"Which direction should I nudge each parameter to reduce the loss?\" Backpropagation Automatically computing all gradients at once Optimizer The strategy for nudging parameters (Adam is a clever one) Inference Using the trained model to generate new text"},{"location":"00-the-big-picture/01-the-200-line-map/","title":"The 200-Line Map","text":""},{"location":"00-the-big-picture/01-the-200-line-map/#the-entire-file-at-a-glance","title":"The Entire File at a Glance","text":"<p>Before we dive into the details, let's look at the whole file from 30,000 feet. Every line of <code>microgpt.py</code> falls into one of six blocks:</p> <pre><code>block-beta\n    columns 3\n    block:header:3\n        columns 3\n        h[\"microgpt.py \u2014 200 lines\"]:3\n    end\n    l1[\"Lines 1-12\"] s1[\"SETUP\"] d1[\"Imports &amp; random seed\"]\n    l2[\"Lines 14-27\"] s2[\"DATA\"] d2[\"Load dataset, build tokenizer\"]\n    l3[\"Lines 29-72\"] s3[\"AUTOGRAD ENGINE\"] d3[\"The Value class\"]\n    l4[\"Lines 74-90\"] s4[\"PARAMETERS\"] d4[\"Initialize model weights\"]\n    l5[\"Lines 92-144\"] s5[\"ARCHITECTURE\"] d5[\"The GPT model function\"]\n    l6[\"Lines 146-184\"] s6[\"TRAINING\"] d6[\"Optimizer + training loop\"]\n    l7[\"Lines 186-200\"] s7[\"INFERENCE\"] d7[\"Generate new text\"]\n\n    style h fill:#1de9b6,stroke:#0db99a,color:#fff\n    style s1 fill:#1de9b6,stroke:#0db99a,color:#000\n    style s2 fill:#1de9b6,stroke:#0db99a,color:#000\n    style s3 fill:#1de9b6,stroke:#0db99a,color:#000\n    style s4 fill:#1de9b6,stroke:#0db99a,color:#000\n    style s5 fill:#1de9b6,stroke:#0db99a,color:#000\n    style s6 fill:#1de9b6,stroke:#0db99a,color:#000\n    style s7 fill:#1de9b6,stroke:#0db99a,color:#000</code></pre> <p>Let's walk through each block.</p>"},{"location":"00-the-big-picture/01-the-200-line-map/#block-1-setup-lines-112","title":"Block 1: Setup (Lines 1\u201312)","text":"microgpt.py \u2014 Lines 1-12<pre><code>import os       # os.path.exists\nimport math     # math.log, math.exp\nimport random   # random.seed, random.choices, random.gauss, random.shuffle\nrandom.seed(42) # Let there be order among chaos\n</code></pre> <p>Three standard Python libraries, zero external dependencies. The <code>random.seed(42)</code> ensures that every time you run the file, you get the same \"random\" numbers \u2014 making experiments reproducible.</p>"},{"location":"00-the-big-picture/01-the-200-line-map/#block-2-data-tokenization-lines-1427","title":"Block 2: Data &amp; Tokenization (Lines 14\u201327)","text":"microgpt.py \u2014 Lines 14-27 (simplified)<pre><code># Download a dataset of names\ndocs = [...]           # list of names like [\"emma\", \"olivia\", ...]\nuchars = sorted(set(...))  # unique characters: ['a', 'b', ..., 'z']\nBOS = len(uchars)      # a special \"start/end of name\" token\nvocab_size = len(uchars) + 1\n</code></pre> <p>Problem \u2192 Solution</p> <p>Problem: How do we turn text into numbers a computer can work with?</p> <p>Solution: Assign each unique character an ID (a=0, b=1, ...) plus one special token.</p> <p> Covered in detail in Module 1</p>"},{"location":"00-the-big-picture/01-the-200-line-map/#block-3-the-autograd-engine-lines-2972","title":"Block 3: The Autograd Engine (Lines 29\u201372)","text":"microgpt.py \u2014 Lines 29-72 (simplified)<pre><code>class Value:\n    def __init__(self, data, children=(), local_grads=()):\n        self.data = data\n        self.grad = 0\n        ...\n    def backward(self):\n        # Automatically compute gradients via the chain rule\n        ...\n</code></pre> <p>Problem \u2192 Solution</p> <p>Problem: How do we figure out which parameters are responsible for errors?</p> <p>Solution: Wrap every number in a <code>Value</code> object that remembers how it was computed. Then walk backwards through the computation to assign blame.</p> <p>This is the heart of the file. The <code>Value</code> class is a tiny automatic differentiation engine \u2014 the same idea behind PyTorch's <code>autograd</code>.</p> <p> Covered in detail in Module 2</p>"},{"location":"00-the-big-picture/01-the-200-line-map/#block-4-parameters-lines-7490","title":"Block 4: Parameters (Lines 74\u201390)","text":"microgpt.py \u2014 Lines 74-90 (simplified)<pre><code>n_embd = 16       # embedding dimension\nn_head = 4        # number of attention heads\nn_layer = 1       # number of layers\nblock_size = 8    # maximum sequence length\n\nstate_dict = {\n    'wte': matrix(...),    # token embeddings\n    'wpe': matrix(...),    # position embeddings\n    'lm_head': matrix(...), # output layer\n    # + attention and MLP weights for each layer\n}\n</code></pre> <p>Problem \u2192 Solution</p> <p>Problem: Where does the model store what it has learned?</p> <p>Solution: In matrices (grids of numbers) that start random and get tuned during training.</p> <p> Covered in detail in Module 3, Lesson 0</p>"},{"location":"00-the-big-picture/01-the-200-line-map/#block-5-the-architecture-lines-92144","title":"Block 5: The Architecture (Lines 92\u2013144)","text":"microgpt.py \u2014 Lines 92-144 (simplified)<pre><code>def gpt(token_id, pos_id, keys, values):\n    # 1. Look up embeddings\n    # 2. For each layer:\n    #    a. Multi-head attention (look at context)\n    #    b. MLP (process information)\n    # 3. Output logits (raw scores for each possible next character)\n    return logits\n</code></pre> <p>Problem \u2192 Solution</p> <p>Problem: Given the current character and position, how do we compute a prediction?</p> <p>Solution: A pipeline of transformations: embed \u2192 normalize \u2192 attend \u2192 think \u2192 predict.</p> <p>This is the Transformer architecture \u2014 the \"T\" in \"GPT\".</p> <p> Covered in detail in Module 3</p>"},{"location":"00-the-big-picture/01-the-200-line-map/#block-6-training-lines-146184","title":"Block 6: Training (Lines 146\u2013184)","text":"microgpt.py \u2014 Lines 146-184 (simplified)<pre><code>for step in range(500):\n    # 1. Pick a name from the dataset\n    # 2. Forward: predict each next character\n    # 3. Measure error (loss)\n    # 4. Backward: compute gradients\n    # 5. Update parameters with Adam optimizer\n</code></pre> <p>Problem \u2192 Solution</p> <p>Problem: How do we make the model better?</p> <p>Solution: Show it examples, measure its mistakes, and nudge its parameters in the right direction. Repeat 500 times.</p> <p> Covered in detail in Module 4</p>"},{"location":"00-the-big-picture/01-the-200-line-map/#block-7-inference-lines-186200","title":"Block 7: Inference (Lines 186\u2013200)","text":"microgpt.py \u2014 Lines 186-200 (simplified)<pre><code>for sample_idx in range(20):\n    # Start with BOS token\n    # Repeatedly: predict next character, pick one, add to output\n    # Stop when BOS is predicted again (end of name)\n    print(f\"sample {sample_idx+1}: {''.join(sample)}\")\n</code></pre> <p>Problem \u2192 Solution</p> <p>Problem: How do we use the trained model to create new names?</p> <p>Solution: Feed it the start signal, let it predict one character at a time, and collect the output.</p> <p> Covered in detail in Module 5</p>"},{"location":"00-the-big-picture/01-the-200-line-map/#the-dependency-chain","title":"The Dependency Chain","text":"<p>The blocks build on each other in a strict order:</p> <pre><code>flowchart TD\n    A[\"\ud83d\udcca Data &amp; Tokenizer\"] --&gt; B[\"\u2699\ufe0f Autograd Engine\"]\n    B --&gt; C[\"\ud83d\udd22 Parameters\"]\n    C --&gt; D[\"\ud83e\udde0 Architecture\"]\n    D --&gt; E[\"\ud83c\udfcb\ufe0f Training\"]\n    E --&gt; F[\"\u2728 Inference\"]\n\n    style A fill:#1de9b6,stroke:#0db99a,color:#fff\n    style B fill:#17c9a0,stroke:#4a2db8,color:#fff\n    style C fill:#12a889,stroke:#0a7d68,color:#fff\n    style D fill:#0d8872,stroke:#085c4e,color:#fff\n    style E fill:#341099,stroke:#240a69,color:#fff\n    style F fill:#1de9b6,stroke:#0db99a,color:#000</code></pre> <p>You can't understand the architecture without understanding autograd. You can't understand training without understanding the architecture. And you can't generate text without a trained model.</p> <p>Course Order</p> <p>This course follows this exact dependency chain. Every lesson builds on the previous one.</p>"},{"location":"00-the-big-picture/02-the-learning-machine-analogy/","title":"The Learning Machine Analogy","text":""},{"location":"00-the-big-picture/02-the-learning-machine-analogy/#a-machine-that-learns-from-mistakes","title":"A Machine That Learns From Mistakes","text":"<p>Before we touch any code, let's build an analogy that will carry us through the entire course.</p>"},{"location":"00-the-big-picture/02-the-learning-machine-analogy/#the-blindfolded-archer","title":"The Blindfolded Archer","text":"<p>Imagine a blindfolded archer trying to hit a target:</p> <pre><code>flowchart LR\n    A[\"\ud83c\udff9 Archer\\n(blindfolded)\"] -- \"shoots\" --&gt; B[\"\u274c Miss!\\n(too high, too right)\"]\n    B -- \"friend says:\\naim lower &amp; left\" --&gt; C[\"\ud83c\udfaf Adjusts aim\"]\n    C -- \"shoots again\" --&gt; A\n\n    style A fill:#1de9b6,stroke:#0db99a,color:#fff\n    style B fill:#64ffda,stroke:#4dd4b0,color:#fff\n    style C fill:#1de9b6,stroke:#0db99a,color:#000</code></pre> <ol> <li>The archer shoots an arrow (makes a prediction)</li> <li>A friend tells them: \"You were 2 meters too high and 1 meter to the right\" (the loss \u2014 how wrong they were)</li> <li>The friend also says: \"Aim lower and more to the left\" (the gradient \u2014 which direction to adjust)</li> <li>The archer adjusts their aim slightly (the parameter update)</li> <li>They shoot again</li> </ol> <p>After hundreds of attempts, the archer is landing arrows near the bullseye \u2014 without ever seeing the target.</p> <p>Key Insight</p> <p>This is exactly how neural networks learn. They never \"see\" the answer directly \u2014 they just get told how wrong they were and which direction to adjust.</p>"},{"location":"00-the-big-picture/02-the-learning-machine-analogy/#mapping-the-analogy-to-code","title":"Mapping the Analogy to Code","text":"Analogy In <code>microgpt.py</code> What it means The archer's aim (angle, force) Parameters (lines 74\u201390) Thousands of numbers that control the model's behavior Shooting an arrow Forward pass (lines 163\u2013168) Running an input through the model to get a prediction \"You missed by X\" Loss (line 169) A single number measuring how bad the prediction was \"Aim lower and left\" Gradients (line 172) The direction to nudge each parameter Adjusting aim Optimizer (lines 174\u2013182) The rule for how much to nudge Shooting again Next training step (line 153) Repeating with the next example"},{"location":"00-the-big-picture/02-the-learning-machine-analogy/#the-three-phases","title":"The Three Phases","text":"<p>The file does three things, in order:</p> Phase 1: Build the MachinePhase 2: Train the MachinePhase 3: Use the Machine <p>Lines 1\u2013144 \u2014 Build the \"archer\": the model that takes an input and produces a prediction.</p> <p>At this point the parameters are random, so the predictions are garbage.</p> <pre><code>Input: \"emm\"  \u2192  Model (random parameters)  \u2192  Prediction: \"q\" \u2190 wrong!\n                                                 (should be \"a\")\n</code></pre> <p>Lines 146\u2013184 \u2014 Show the model thousands of real names. For each one:</p> <ul> <li>Let it predict the next character</li> <li>Tell it how wrong it was</li> <li>Adjust parameters slightly</li> </ul> <pre><code>Step    1: loss = 3.8912  (predictions are random garbage)\nStep  100: loss = 2.4561  (starting to learn common patterns)\nStep  300: loss = 1.8234  (getting the hang of it)\nStep  500: loss = 1.5012  (reasonably good at predicting)\n</code></pre> <p>The loss going down means the model is getting better.</p> <p>Lines 186\u2013200 \u2014 Now that the parameters have been tuned, we can use the model to generate new names it has never seen:</p> <pre><code>sample  1: emma\nsample  2: ariel\nsample  3: kaya\nsample  4: suri\nsample  5: livia\n</code></pre> <p>These names didn't exist in the training data \u2014 the model invented them by learning the patterns of what makes a name \"name-like.\"</p>"},{"location":"00-the-big-picture/02-the-learning-machine-analogy/#why-everything-else-is-just-efficiency","title":"Why \"Everything Else Is Just Efficiency\"","text":"<p>Karpathy's claim</p> <p>\"This is the complete algorithm. Everything else is just efficiency.\"</p> <p>What does he mean? This 200-line file contains:</p> What Present in microgpt.py? What the \"real world\" adds Tokenization Faster tokenizers (BPE) with larger vocabularies Autograd GPU-accelerated autograd (PyTorch/JAX) Transformer architecture More layers, bigger embeddings, but same structure Attention mechanism FlashAttention (same math, faster execution) Training loop Distributed training across thousands of GPUs Adam optimizer Same optimizer, just parallelized Text generation Same algorithm with beam search, nucleus sampling <p>The algorithm is identical. What changes at scale is:</p> <ul> <li>Speed: GPUs instead of Python loops</li> <li>Size: Billions of parameters instead of thousands</li> <li>Data: Terabytes of text instead of a names file</li> </ul> <p>But the logic \u2014 embed, attend, predict, measure error, compute gradients, update \u2014 is the same logic you'll learn in this course.</p>"},{"location":"00-the-big-picture/02-the-learning-machine-analogy/#the-road-ahead","title":"The Road Ahead","text":"<pre><code>flowchart TD\n    HERE[\"\ud83d\udccd You are here\"] --&gt; M1\n    M1[\"Module 1: How do we get data\\nand turn it into numbers?\"] --&gt; M2\n    M2[\"Module 2: How do we automatically\\nfind 'which way to nudge'?\"] --&gt; M3\n    M3[\"Module 3: What math does the model\\nactually do on the numbers?\"] --&gt; M4\n    M4[\"Module 4: How does the\\ntraining loop work?\"] --&gt; M5\n    M5[\"Module 5: How do we\\ngenerate new text?\"] --&gt; DONE\n    DONE[\"\u2705 You understand the entire file\"]\n\n    style HERE fill:#64ffda,stroke:#4dd4b0,color:#fff\n    style M1 fill:#1de9b6,stroke:#0db99a,color:#fff\n    style M2 fill:#17c9a0,stroke:#4a2db8,color:#fff\n    style M3 fill:#12a889,stroke:#0a7d68,color:#fff\n    style M4 fill:#0d8872,stroke:#085c4e,color:#fff\n    style M5 fill:#341099,stroke:#240a69,color:#fff\n    style DONE fill:#1de9b6,stroke:#0db99a,color:#000</code></pre>"},{"location":"01-data-and-tokenization/00-the-dataset/","title":"The Dataset","text":""},{"location":"01-data-and-tokenization/00-the-dataset/#the-problem","title":"The Problem","text":"<p>We want to build a model that generates human-like names. But before we can do any math, we need examples for the model to learn from.</p> <p>Think of it this way</p> <p>If you wanted to learn what names look like in a foreign language, you'd need a list of names in that language to study. The model needs the same thing.</p>"},{"location":"01-data-and-tokenization/00-the-dataset/#the-data-lines-1421","title":"The Data (Lines 14\u201321)","text":"<p>Here's how <code>microgpt.py</code> gets its training data:</p> microgpt.py \u2014 Lines 14-21<pre><code>if not os.path.exists('input.txt'):\n    import urllib.request\n    names_url = 'https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt'\n    urllib.request.urlretrieve(names_url, 'input.txt')\ndocs = [l.strip() for l in open('input.txt').read().strip().split('\\n') if l.strip()]\nrandom.shuffle(docs)\nprint(f\"num docs: {len(docs)}\")\n</code></pre> <p>Let's break this down line by line.</p>"},{"location":"01-data-and-tokenization/00-the-dataset/#lines-15-18-download-if-missing","title":"Lines 15-18: Download if missing","text":"microgpt.py \u2014 Lines 15-18<pre><code>if not os.path.exists('input.txt'):\n    import urllib.request\n    names_url = 'https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt'\n    urllib.request.urlretrieve(names_url, 'input.txt')\n</code></pre> <p>Checks if the file <code>input.txt</code> already exists. If not, it downloads a list of ~32,000 human names from the internet and saves it locally. Simple.</p>"},{"location":"01-data-and-tokenization/00-the-dataset/#line-19-load-and-clean","title":"Line 19: Load and clean","text":"microgpt.py \u2014 Line 19<pre><code>docs = [l.strip() for l in open('input.txt').read().strip().split('\\n') if l.strip()]\n</code></pre> <p>This is a dense one-liner. Let's unpack it step by step:</p> Step 1: ReadStep 2: StripStep 3: SplitStep 4: Filter <pre><code>open('input.txt').read()  # \u2192 reads the ENTIRE file as one big string\n</code></pre> <pre><code>.strip()  # \u2192 removes whitespace from the start and end\n</code></pre> <pre><code>.split('\\n')  # \u2192 splits by newlines \u2192 list of lines\n</code></pre> <pre><code>[l.strip() for l in ... if l.strip()]  # \u2192 strip each line, skip empties\n</code></pre> <p>The result: <code>docs</code> is a Python list of strings:</p> <pre><code>docs = [\"emma\", \"olivia\", \"ava\", \"isabella\", \"sophia\", ...]\n</code></pre> <p>Each string is one \"document.\" In our case, a document is a single name.</p>"},{"location":"01-data-and-tokenization/00-the-dataset/#line-20-shuffle","title":"Line 20: Shuffle","text":"microgpt.py \u2014 Line 20<pre><code>random.shuffle(docs)\n</code></pre> <p>Why shuffle?</p> <p>If the model saw all names starting with \"a\" first, then all \"b\" names, etc., it might learn the wrong patterns. Shuffling ensures the model sees a diverse mix at each training step.</p>"},{"location":"01-data-and-tokenization/00-the-dataset/#line-21-print-the-count","title":"Line 21: Print the count","text":"microgpt.py \u2014 Line 21<pre><code>print(f\"num docs: {len(docs)}\")  # Output: num docs: 32033\n</code></pre> <p>About 32,000 names. That's our entire dataset.</p>"},{"location":"01-data-and-tokenization/00-the-dataset/#why-this-matters","title":"Why This Matters","text":"<p>Important</p> <p>The dataset is the source of truth. The model will never know anything that isn't somehow present in this data. If we fed it city names instead, it would generate city-like names. If we fed it Python code, it would generate code-like text.</p> <p>The model learns patterns from data. No data = no learning.</p> Terminology Term Meaning Document A single training example. Here, one name (e.g., \"emma\") Dataset The full collection of documents Corpus Another word for dataset (you'll see this in papers) Shuffle Randomize the order of documents before training"},{"location":"01-data-and-tokenization/01-characters-as-numbers/","title":"Characters as Numbers","text":""},{"location":"01-data-and-tokenization/01-characters-as-numbers/#the-problem","title":"The Problem","text":"<p>We have a list of names like <code>[\"emma\", \"olivia\", ...]</code>. But computers can only do math on numbers. We need a way to convert characters to numbers and back.</p> <p>This conversion process is called tokenization.</p>"},{"location":"01-data-and-tokenization/01-characters-as-numbers/#what-is-a-token","title":"What Is a Token?","text":"<p>Definition</p> <p>A token is the smallest unit the model works with. In our case, each token is a single character. In larger models (like ChatGPT), tokens are parts of words (like \"un\", \"believ\", \"able\").</p> <p>Using characters keeps things simple: there are only ~27 unique characters in our names dataset.</p>"},{"location":"01-data-and-tokenization/01-characters-as-numbers/#the-code-lines-2327","title":"The Code (Lines 23\u201327)","text":"microgpt.py \u2014 Lines 23-27<pre><code>uchars = sorted(set(''.join(docs)))  # unique characters in the dataset\nBOS = len(uchars)                     # token id for Beginning of Sequence\nvocab_size = len(uchars) + 1          # total unique tokens (+1 for BOS)\nprint(f\"vocab size: {vocab_size}\")\n</code></pre>"},{"location":"01-data-and-tokenization/01-characters-as-numbers/#line-24-building-the-vocabulary","title":"Line 24: Building the vocabulary","text":"microgpt.py \u2014 Line 24<pre><code>uchars = sorted(set(''.join(docs)))\n</code></pre> <p>Let's trace this step by step:</p> Step 1: JoinStep 2: UniqueStep 3: Sort <pre><code>''.join(docs)  # \u2192 \"emmaoliviaava...\" (all names glued together)\n</code></pre> <pre><code>set(...)  # \u2192 {'e', 'm', 'a', 'o', 'l', 'i', 'v', ...}  (unique chars only)\n</code></pre> <pre><code>sorted(...)  # \u2192 ['a', 'b', 'c', 'd', ..., 'z']  (sorted alphabetically)\n</code></pre> <p>The result is a sorted list of every unique character:</p> <pre><code>uchars = ['a', 'b', 'c', 'd', 'e', ..., 'y', 'z']\n# index:   0    1    2    3    4   ...   24   25\n</code></pre> <p>The index of each character in this list becomes its token ID:</p> <ul> <li><code>'a'</code> \u2192 0</li> <li><code>'b'</code> \u2192 1</li> <li><code>'e'</code> \u2192 4</li> <li><code>'z'</code> \u2192 25</li> </ul>"},{"location":"01-data-and-tokenization/01-characters-as-numbers/#encoding-and-decoding","title":"Encoding and Decoding","text":"<pre><code>flowchart LR\n    A[\"'e'\"] -- \"uchars.index('e')\" --&gt; B[\"4\"]\n    B -- \"uchars[4]\" --&gt; A\n\n    style A fill:#1de9b6,stroke:#0db99a,color:#fff\n    style B fill:#1de9b6,stroke:#0db99a,color:#000</code></pre> <p>The name \"emma\" becomes:</p> Character Token ID <code>'e'</code> 4 <code>'m'</code> 12 <code>'m'</code> 12 <code>'a'</code> 0 \\[\\text{\"emma\"} \\rightarrow [4, 12, 12, 0]\\] <p>And \\([4, 12, 12, 0]\\) can be decoded back to <code>\"emma\"</code>.</p>"},{"location":"01-data-and-tokenization/01-characters-as-numbers/#why-sorted","title":"Why Sorted?","text":"<p>Sorting (<code>sorted(...)</code>) isn't strictly necessary \u2014 any consistent mapping would work. But sorting makes the mapping deterministic and predictable, which helps with debugging.</p>"},{"location":"01-data-and-tokenization/01-characters-as-numbers/#why-not-just-use-ascii","title":"Why Not Just Use ASCII?","text":"<p>Good question</p> <p>Characters already have numbers assigned to them (ASCII codes: a=97, b=98, ...). Why not use those?</p> <ol> <li>Wasted space: ASCII has 128 codes, but we only use ~27 characters. Our model would have to learn about 100+ unused tokens, wasting parameters.</li> <li>Contiguous IDs: We want IDs from 0 to \\(N-1\\) with no gaps, so they can directly index into arrays and matrices.</li> </ol> Terminology Term Meaning Token The smallest unit the model processes (here: a character) Tokenizer The system that converts between text and token IDs Vocabulary The complete set of all possible tokens vocab_size How many unique tokens exist (here: 27 = 26 letters + BOS) Encoding Converting text \u2192 token IDs Decoding Converting token IDs \u2192 text"},{"location":"01-data-and-tokenization/02-the-bos-token/","title":"The BOS Token","text":""},{"location":"01-data-and-tokenization/02-the-bos-token/#the-problem","title":"The Problem","text":"<p>Suppose the model is learning from the name \"emma\". We train it on these predictions:</p> Given Predict <code>'e'</code> <code>'m'</code> <code>'m'</code> <code>'m'</code> <code>'m'</code> <code>'a'</code> <p>But two critical things are missing:</p> <p>Two missing pieces</p> <ol> <li>How does the model know to start with 'e'? Something has to come before the first character.</li> <li>How does the model know to stop after 'a'? It could keep generating characters forever.</li> </ol> <p>We need a way to say: \"This is the beginning\" and \"This is the end.\"</p>"},{"location":"01-data-and-tokenization/02-the-bos-token/#the-solution-bos-beginning-of-sequence","title":"The Solution: BOS (Beginning of Sequence)","text":"<p><code>microgpt.py</code> uses a single special token called BOS for both purposes:</p> microgpt.py \u2014 Lines 25-26<pre><code>BOS = len(uchars)      # token id 26 (one past the last character)\nvocab_size = len(uchars) + 1  # 27 total tokens\n</code></pre> <p><code>BOS</code> gets the token ID <code>26</code> \u2014 the next available number after 'z' (which is 25).</p>"},{"location":"01-data-and-tokenization/02-the-bos-token/#how-bos-works","title":"How BOS Works","text":"<p>When preparing a name for training, the code wraps it with BOS on both sides:</p> microgpt.py \u2014 Line 157<pre><code>tokens = [BOS] + [uchars.index(ch) for ch in doc] + [BOS]\n</code></pre> <p>For the name \"emma\":</p> <pre><code>flowchart LR\n    B1[\"BOS\\n(26)\"] --&gt; E[\"e\\n(4)\"] --&gt; M1[\"m\\n(12)\"] --&gt; M2[\"m\\n(12)\"] --&gt; A[\"a\\n(0)\"] --&gt; B2[\"BOS\\n(26)\"]\n\n    style B1 fill:#64ffda,stroke:#4dd4b0,color:#fff\n    style E fill:#1de9b6,stroke:#0db99a,color:#fff\n    style M1 fill:#1de9b6,stroke:#0db99a,color:#fff\n    style M2 fill:#1de9b6,stroke:#0db99a,color:#fff\n    style A fill:#1de9b6,stroke:#0db99a,color:#fff\n    style B2 fill:#64ffda,stroke:#4dd4b0,color:#fff</code></pre> <p>Now the training pairs become:</p> Input \u2192 Target Meaning BOS \u2192 <code>'e'</code> \"After the start signal, predict 'e'\" <code>'e'</code> \u2192 <code>'m'</code> \"After 'e', predict 'm'\" <code>'m'</code> \u2192 <code>'m'</code> \"After 'm', predict 'm'\" <code>'m'</code> \u2192 <code>'a'</code> \"After 'm', predict 'a'\" <code>'a'</code> \u2192 BOS \"After 'a', predict the STOP signal\" <p>This solves both problems:</p> <ul> <li>Starting: The model learns what characters are likely after BOS (i.e., which characters names typically start with)</li> <li>Stopping: The model learns when to produce BOS (i.e., when the name should end)</li> </ul>"},{"location":"01-data-and-tokenization/02-the-bos-token/#why-one-token-for-both","title":"Why One Token for Both?","text":"<p>Elegant design</p> <p>Using the same token for start and stop is elegant:</p> <ul> <li>It means fewer special tokens (smaller vocabulary)</li> <li>During generation, we feed BOS to start, and stop when the model produces BOS</li> <li>Mathematically, it doesn't matter \u2014 the model learns from context whether BOS means \"start\" or \"stop\"</li> </ul>"},{"location":"01-data-and-tokenization/02-the-bos-token/#the-complete-token-system","title":"The Complete Token System","text":"Token ID Character Type 0 <code>'a'</code> regular 1 <code>'b'</code> regular ... ... regular 25 <code>'z'</code> regular 26 <code>&lt;BOS&gt;</code> special vocab_size = 27"},{"location":"01-data-and-tokenization/02-the-bos-token/#checkpoint","title":"Checkpoint \u2713","text":"<p>What we know so far</p> <ul> <li> How to load a dataset of names</li> <li> How to convert characters to numbers (tokenization)</li> <li> How to mark the start and end of each name (BOS token)</li> </ul> <p>What we don't know yet: how does the model actually use these numbers to make predictions? For that, we need some math \u2014 specifically, we need a way to figure out \"which direction to nudge\" after making a wrong prediction.</p> Terminology Term Meaning BOS Beginning of Sequence \u2014 a special token marking the start (and end) of a document Special token A token that doesn't represent a real character; it's a control signal Sequence An ordered list of tokens Wrapping Adding special tokens around a document before training"},{"location":"02-calculus-and-autograd/00-why-we-need-derivatives/","title":"Why We Need Derivatives","text":""},{"location":"02-calculus-and-autograd/00-why-we-need-derivatives/#the-problem","title":"The Problem","text":"<p>We have a model that takes a character and produces a prediction for the next character. But right now all its parameters are random, so its predictions are garbage.</p> <p>How do we improve it?</p>"},{"location":"02-calculus-and-autograd/00-why-we-need-derivatives/#the-naive-approach-random-search","title":"The Naive Approach: Random Search","text":"<p>One idea: randomly change each parameter, check if the loss went down, and keep the change if it did.</p> <pre><code>Parameter p = 0.5\nLoss = 3.2\n\nTry p = 0.51  \u2192 Loss = 3.1 \u2713  (keep it!)\nTry p = 0.52  \u2192 Loss = 3.3 \u2717  (revert!)\nTry p = 0.49  \u2192 Loss = 3.0 \u2713  (keep it!)\n...\n</code></pre> <p>This works! But it's hopelessly slow. If you have 10,000 parameters, and you try 10 variations each, that's 100,000 forward passes per step. At 500 training steps, that's 50 million forward passes. We'd be here forever.</p>"},{"location":"02-calculus-and-autograd/00-why-we-need-derivatives/#the-key-insight","title":"The Key Insight","text":"<p>The Big Idea</p> <p>What if, instead of guessing, we could calculate which direction to nudge each parameter \u2014 all at once, in a single pass?</p> <p>That's exactly what derivatives give us.</p>"},{"location":"02-calculus-and-autograd/00-why-we-need-derivatives/#what-is-a-derivative","title":"What Is a Derivative?","text":"<p>A derivative answers one question:</p> <p>If I change this input by a tiny amount, how much does the output change?</p>"},{"location":"02-calculus-and-autograd/00-why-we-need-derivatives/#example-a-simple-function","title":"Example: A Simple Function","text":"\\[f(x) = x^2\\] \\(x\\) \\(f(x) = x^2\\) 1 1 2 4 3 9 4 16 <p>What happens when we nudge \\(x\\) from 3 to 3.001?</p> \\[f(3) = 9\\] \\[f(3.001) = 9.006001\\] <p>The output changed by ~0.006 when we changed the input by 0.001. The rate of change is:</p> \\[\\frac{\\Delta f}{\\Delta x} = \\frac{0.006001}{0.001} = 6.001 \\approx 6\\] <p>This rate of change is the derivative. For \\(f(x) = x^2\\), the derivative is \\(f'(x) = 2x\\). At \\(x = 3\\), that's \\(2 \\times 3 = 6\\). </p>"},{"location":"02-calculus-and-autograd/00-why-we-need-derivatives/#the-derivative-as-a-direction-sign","title":"The Derivative as a Direction Sign","text":"<p>The derivative tells you two things:</p> Direction (sign)Sensitivity (magnitude) <ul> <li>Positive derivative \u2192 increasing \\(x\\) increases the output</li> <li>Negative derivative \u2192 increasing \\(x\\) decreases the output</li> </ul> <ul> <li>Large absolute value \u2192 this parameter matters a lot</li> <li>Small absolute value \u2192 this parameter barely matters</li> </ul>"},{"location":"02-calculus-and-autograd/00-why-we-need-derivatives/#why-this-matters-for-training","title":"Why This Matters for Training","text":"<p>In training, we want to minimize the loss (the error). If we know the derivative of the loss with respect to each parameter:</p> <p>The Update Rule</p> \\[\\text{derivative is positive} \\implies \\text{parameter is pushing loss UP} \\implies \\text{DECREASE it}\\] \\[\\text{derivative is negative} \\implies \\text{parameter is pushing loss DOWN} \\implies \\text{INCREASE it}\\] <p>The update rule is beautifully simple:</p> \\[\\text{parameter} = \\text{parameter} - \\text{learning\\_rate} \\times \\text{derivative}\\] <p>The minus sign flips the direction \u2014 we always move opposite to the derivative, because we want to go downhill on the loss.</p>"},{"location":"02-calculus-and-autograd/00-why-we-need-derivatives/#the-hill-analogy","title":"The Hill Analogy","text":"<p>Imagine you're on a hilly landscape in fog (you can't see the valley). The loss is your altitude. The derivative tells you the slope under your feet.</p> <pre><code>flowchart LR\n    A[\"\ud83c\udfd4\ufe0f High Loss\\n(positive slope)\"] -- \"step opposite\\nto slope\" --&gt; B[\"\ud83d\udccd You are here\"]\n    B -- \"step opposite\\nto slope\" --&gt; C[\"\ud83c\udfde\ufe0f Low Loss\\n(the valley)\"]\n\n    style A fill:#64ffda,stroke:#4dd4b0,color:#fff\n    style B fill:#80ffe5,stroke:#5cd4bc,color:#fff\n    style C fill:#1de9b6,stroke:#0db99a,color:#000</code></pre> <ul> <li>Positive slope \u2192 you're going uphill \u2192 step in the negative direction</li> <li>Negative slope \u2192 you're going downhill \u2192 keep going</li> </ul> <p>Each small step takes you closer to the valley.</p>"},{"location":"02-calculus-and-autograd/00-why-we-need-derivatives/#derivatives-we-need-to-know","title":"Derivatives We Need to Know","text":"<p>For <code>microgpt.py</code>, we only need the derivatives of a few basic operations:</p> Operation Formula Derivative w.r.t. \\(x\\) Addition \\(x + y\\) \\(1\\) Multiplication \\(x \\times y\\) \\(y\\) Power \\(x^n\\) \\(n \\cdot x^{n-1}\\) Exponential \\(e^x\\) \\(e^x\\) Logarithm \\(\\ln(x)\\) \\(\\frac{1}{x}\\) ReLU \\(\\max(0, x)\\) \\(1\\) if \\(x &gt; 0\\), else \\(0\\) <p>Note</p> <p>Don't memorize these. They'll come up naturally when we look at the <code>Value</code> class, and we'll explain each one in context.</p>"},{"location":"02-calculus-and-autograd/00-why-we-need-derivatives/#but-theres-a-catch","title":"But There's a Catch","text":"<p>Real models aren't just \\(f(x) = x^2\\). They're compositions of hundreds of operations:</p> <pre><code>loss = log(softmax(linear(relu(linear(embed(token))))))\n</code></pre> <p>How do we find the derivative of the loss with respect to a parameter buried deep inside <code>embed()</code>?</p> <p>That's where the chain rule comes in.</p>"},{"location":"02-calculus-and-autograd/01-the-chain-rule/","title":"The Chain Rule","text":""},{"location":"02-calculus-and-autograd/01-the-chain-rule/#the-problem","title":"The Problem","text":"<p>We know derivatives tell us \"which way to nudge.\" But in a neural network, the loss depends on parameters through a long chain of operations:</p> <pre><code>flowchart LR\n    A[\"parameter\"] --&gt; B[\"embedding\"] --&gt; C[\"linear\"] --&gt; D[\"relu\"] --&gt; E[\"linear\"] --&gt; F[\"softmax\"] --&gt; G[\"log\"] --&gt; H[\"loss\"]\n\n    style A fill:#1de9b6,stroke:#0db99a,color:#fff\n    style H fill:#64ffda,stroke:#4dd4b0,color:#fff</code></pre> <p>How do we find the derivative of <code>loss</code> with respect to <code>parameter</code> when there are 6 operations between them?</p>"},{"location":"02-calculus-and-autograd/01-the-chain-rule/#composition-of-functions","title":"Composition of Functions","text":"<p>When you nest functions inside functions, it's called composition:</p> \\[\\text{Simple: } f(x) = x^2 \\quad \\text{(one step)}\\] \\[\\text{Composed: } h(x) = f(g(x)) \\quad \\text{where } g(x) = 2x + 1 \\text{ and } f(z) = z^2\\] \\[\\text{So } h(x) = (2x + 1)^2 \\quad \\text{(two steps)}\\] <p>For \\(h(x) = (2x + 1)^2\\):</p> <ol> <li>First compute \\(g = 2x + 1\\)</li> <li>Then compute \\(f = g^2\\)</li> </ol> <p>The question is: what is the derivative of \\(h\\) with respect to \\(x\\)?</p>"},{"location":"02-calculus-and-autograd/01-the-chain-rule/#the-chain-rule_1","title":"The Chain Rule","text":"<p>The Chain Rule</p> <p>The derivative of a composition is the product of the individual derivatives.</p> \\[\\frac{dh}{dx} = \\frac{df}{dg} \\times \\frac{dg}{dx}\\] <p>Let's verify with numbers. For \\(h(x) = (2x + 1)^2\\) at \\(x = 3\\):</p> Step 1: Individual derivativesStep 2: Chain ruleStep 3: Verify \\[g(x) = 2x + 1 \\implies \\frac{dg}{dx} = 2\\] \\[f(g) = g^2 \\implies \\frac{df}{dg} = 2g = 2(2x + 1)\\] \\[\\frac{dh}{dx} = \\frac{df}{dg} \\times \\frac{dg}{dx} = 2(2 \\times 3 + 1) \\times 2 = 2(7) \\times 2 = 28\\] <p>Nudge \\(x\\) from 3 to 3.001:</p> \\[h(3) = (2 \\times 3 + 1)^2 = 7^2 = 49\\] \\[h(3.001) = (2 \\times 3.001 + 1)^2 = 7.002^2 = 49.028004\\] \\[\\frac{\\Delta h}{\\Delta x} = \\frac{0.028004}{0.001} = 28.004 \\approx 28 \\checkmark\\]"},{"location":"02-calculus-and-autograd/01-the-chain-rule/#the-chain-rule-visually","title":"The Chain Rule Visually","text":"<p>Think of it like a pipeline. Each stage multiplies the \"sensitivity\":</p> <pre><code>flowchart LR\n    X[\"x\"] -- \"\u00d72\" --&gt; G[\"g\"] -- \"\u00d72g\" --&gt; H[\"h\"]\n\n    style X fill:#1de9b6,stroke:#0db99a,color:#fff\n    style G fill:#12a889,stroke:#0a7d68,color:#fff\n    style H fill:#1de9b6,stroke:#0db99a,color:#000</code></pre> <p>If \\(x\\) wiggles by 1:</p> <ul> <li>\\(g\\) wiggles by 2 (because \\(\\frac{dg}{dx} = 2\\))</li> <li>\\(h\\) wiggles by \\(2 \\times 2g\\) (because \\(\\frac{df}{dg} = 2g\\), and \\(g\\) already wiggled by 2)</li> </ul>"},{"location":"02-calculus-and-autograd/01-the-chain-rule/#longer-chains","title":"Longer Chains","text":"<p>The chain rule extends to any number of steps:</p> \\[f(g(h(x))) \\implies \\frac{df}{dx} = \\frac{df}{dg} \\times \\frac{dg}{dh} \\times \\frac{dh}{dx}\\] <p>Just multiply all the individual derivatives along the chain.</p> <p>Info</p> <p>In a neural network, this chain might be 10 or 100 steps long. But the principle is always the same: multiply the local derivatives along the path.</p>"},{"location":"02-calculus-and-autograd/01-the-chain-rule/#the-key-insight-for-autograd","title":"The Key Insight for Autograd","text":"<p>This is why <code>microgpt.py</code>'s <code>Value</code> class stores local gradients at each operation:</p> Operation Local gradient of \\(z\\) w.r.t. \\(x\\) \\(z = x + y\\) \\(1\\) \\(z = x \\times y\\) \\(y\\) \\(z = x^2\\) \\(2x\\) <p>Each operation only needs to know its own local derivative. The chain rule takes care of composing them into the full derivative.</p>"},{"location":"02-calculus-and-autograd/01-the-chain-rule/#a-three-node-example","title":"A Three-Node Example","text":"<p>Let's trace through a tiny computation graph:</p> <pre><code>a = 2.0\nb = 3.0\nc = a \u00d7 b    # c = 6.0\nd = c + 1    # d = 7.0\nloss = d\u00b2    # loss = 49.0\n</code></pre> <p>We want: \\(\\frac{d(\\text{loss})}{da}\\) \u2014 how does the loss change if we tweak \\(a\\)?</p> \\[\\frac{d(\\text{loss})}{da} = \\frac{d(\\text{loss})}{dd} \\times \\frac{dd}{dc} \\times \\frac{dc}{da} = 2d \\times 1 \\times b = 2(7) \\times 1 \\times 3 = 42\\] <p>Each factor is a local gradient \u2014 the derivative of one node with respect to its immediate input. The chain rule multiplies them together.</p> Terminology Term Meaning Chain rule \\(\\frac{d(f \\circ g)}{dx} = \\frac{df}{dg} \\times \\frac{dg}{dx}\\) \u2014 multiply local derivatives Local gradient The derivative of one operation w.r.t. its immediate input Composition Nesting functions: \\(f(g(x))\\) Sensitivity How much the output changes when an input wiggles"},{"location":"02-calculus-and-autograd/02-the-value-class/","title":"The Value Class","text":""},{"location":"02-calculus-and-autograd/02-the-value-class/#the-problem","title":"The Problem","text":"<p>We know the chain rule lets us compute derivatives through a chain of operations. But doing this by hand for thousands of parameters and hundreds of operations is impossible.</p> <p>We need the computer to do it automatically. That's called automatic differentiation (autograd for short).</p> <p>The Trick</p> <p>Every time we do a math operation, we record what we did and how to differentiate it. Then at the end, we replay the recording backwards to compute all derivatives at once.</p>"},{"location":"02-calculus-and-autograd/02-the-value-class/#the-value-class-lines-3037","title":"The Value Class (Lines 30\u201337)","text":"<p>Here is the core data structure:</p> microgpt.py \u2014 Lines 30-37<pre><code>class Value:\n    \"\"\"Stores a single scalar value and its gradient, as a node in a computation graph.\"\"\"\n\n    def __init__(self, data, children=(), local_grads=()):\n        self.data = data                # scalar value of this node\n        self.grad = 0                   # derivative of the loss w.r.t. this node\n        self._children = children       # children of this node in the computation graph\n        self._local_grads = local_grads # local derivative of this node w.r.t. its children\n</code></pre> <p>Every number in the model is wrapped in a <code>Value</code> object. Each <code>Value</code> stores four things:</p> Attribute What it stores Example <code>data</code> The actual number 3.14 <code>grad</code> How much the loss changes if this number changes Filled in during backward pass <code>_children</code> Which <code>Value</code>s were combined to produce this one <code>(a, b)</code> if this value = a + b <code>_local_grads</code> The derivative of this operation w.r.t. each child <code>(1, 1)</code> for addition"},{"location":"02-calculus-and-autograd/02-the-value-class/#addition-lines-3941","title":"Addition (Lines 39\u201341)","text":"microgpt.py \u2014 Lines 39-41<pre><code>def __add__(self, other):\n    other = other if isinstance(other, Value) else Value(other)\n    return Value(self.data + other.data, (self, other), (1, 1))\n</code></pre> <p>When you write <code>c = a + b</code> where <code>a</code> and <code>b</code> are <code>Value</code> objects:</p> <ol> <li><code>self.data + other.data</code> \u2192 compute the result (forward pass)</li> <li><code>(self, other)</code> \u2192 remember the children (a and b)</li> <li><code>(1, 1)</code> \u2192 store the local gradients</li> </ol> <p>Why <code>(1, 1)</code> for addition? Because:</p> \\[c = a + b \\implies \\frac{dc}{da} = 1, \\quad \\frac{dc}{db} = 1\\] <p>Changing \\(a\\) by 1 changes \\(c\\) by 1. Same for \\(b\\).</p> <p>Example</p> <pre><code>a = Value(2.0)\nb = Value(3.0)\nc = a + b  # c.data = 5.0, c._children = (a, b), c._local_grads = (1, 1)\n</code></pre> <pre><code>flowchart BT\n    A[\"a (2.0)\"] -- \"grad = 1\" --&gt; C[\"c (5.0)\"]\n    B[\"b (3.0)\"] -- \"grad = 1\" --&gt; C\n\n    style C fill:#1de9b6,stroke:#0db99a,color:#000\n    style A fill:#1de9b6,stroke:#0db99a,color:#fff\n    style B fill:#1de9b6,stroke:#0db99a,color:#fff</code></pre>"},{"location":"02-calculus-and-autograd/02-the-value-class/#multiplication-lines-4345","title":"Multiplication (Lines 43\u201345)","text":"microgpt.py \u2014 Lines 43-45<pre><code>def __mul__(self, other):\n    other = other if isinstance(other, Value) else Value(other)\n    return Value(self.data * other.data, (self, other), (other.data, self.data))\n</code></pre> <p>For \\(c = a \\times b\\):</p> \\[\\frac{dc}{da} = b, \\quad \\frac{dc}{db} = a\\] <p>So the local gradients are <code>(other.data, self.data)</code> \u2014 each child's gradient is the other child's value.</p> <p>Example \u2014 Notice the swap!</p> <pre><code>a = Value(2.0)\nb = Value(3.0)\nc = a * b  # c.data = 6.0, c._local_grads = (3.0, 2.0)\n</code></pre> <pre><code>flowchart BT\n    A[\"a (2.0)\"] -- \"grad = 3.0 \u2190 b's value!\" --&gt; C[\"c (6.0)\"]\n    B[\"b (3.0)\"] -- \"grad = 2.0 \u2190 a's value!\" --&gt; C\n\n    style C fill:#1de9b6,stroke:#0db99a,color:#000\n    style A fill:#1de9b6,stroke:#0db99a,color:#fff\n    style B fill:#1de9b6,stroke:#0db99a,color:#fff</code></pre> <p>Why swapped? If you're multiplying \\(2 \\times 3\\) and increase the 2 to 3, you get \\(3 \\times 3 = 9\\). The result changed by 3 (which is the other number).</p>"},{"location":"02-calculus-and-autograd/02-the-value-class/#power-line-47","title":"Power (Line 47)","text":"microgpt.py \u2014 Line 47<pre><code>def __pow__(self, other):\n    return Value(self.data**other, (self,), (other * self.data**(other-1),))\n</code></pre> \\[c = a^n \\implies \\frac{dc}{da} = n \\cdot a^{n-1}\\] <p>This is the power rule from calculus. Note that <code>other</code> here is a plain number, not a <code>Value</code>.</p> <p>Example: \\(a^2\\) at \\(a = 3\\)</p> \\[\\frac{dc}{da} = 2 \\times 3^{(2-1)} = 2 \\times 3 = 6\\]"},{"location":"02-calculus-and-autograd/02-the-value-class/#logarithm-line-48","title":"Logarithm (Line 48)","text":"microgpt.py \u2014 Line 48<pre><code>def log(self):\n    return Value(math.log(self.data), (self,), (1/self.data,))\n</code></pre> \\[c = \\ln(a) \\implies \\frac{dc}{da} = \\frac{1}{a}\\] <p>This is used in the loss function: \\(-\\log(\\text{probability})\\). If the probability is 0.5, the gradient is \\(\\frac{1}{0.5} = 2\\).</p>"},{"location":"02-calculus-and-autograd/02-the-value-class/#exponential-line-49","title":"Exponential (Line 49)","text":"microgpt.py \u2014 Line 49<pre><code>def exp(self):\n    return Value(math.exp(self.data), (self,), (math.exp(self.data),))\n</code></pre> \\[c = e^a \\implies \\frac{dc}{da} = e^a\\] <p>Beautiful fact</p> <p>The exponential function is its own derivative \u2014 one of the most elegant facts in math. This is used in the softmax function.</p>"},{"location":"02-calculus-and-autograd/02-the-value-class/#relu-line-50","title":"ReLU (Line 50)","text":"microgpt.py \u2014 Line 50<pre><code>def relu(self):\n    return Value(max(0, self.data), (self,), (float(self.data &gt; 0),))\n</code></pre> <p>ReLU (Rectified Linear Unit) is the simplest \"activation function\":</p> \\[\\text{relu}(x) = \\max(0, x) = \\begin{cases} x &amp; \\text{if } x &gt; 0 \\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] <p>Its derivative:</p> \\[\\frac{d(\\text{relu})}{dx} = \\begin{cases} 1 &amp; \\text{if } x &gt; 0 \\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] <p>If the input is positive, the gradient flows through unchanged. If negative, the gradient is zero \u2014 the operation is \"dead.\"</p>"},{"location":"02-calculus-and-autograd/02-the-value-class/#convenience-operations-lines-5157","title":"Convenience Operations (Lines 51\u201357)","text":"microgpt.py \u2014 Lines 51-57<pre><code>def __neg__(self): return self * -1\ndef __radd__(self, other): return self + other\ndef __sub__(self, other): return self + (-other)\ndef __rsub__(self, other): return other + (-self)\ndef __rmul__(self, other): return self * other\ndef __truediv__(self, other): return self * other**-1\ndef __rtruediv__(self, other): return other * self**-1\n</code></pre> <p>These define subtraction, division, and negation in terms of the primitives we already have:</p> Operation Implemented as \\(-a\\) <code>a * -1</code> \\(a - b\\) <code>a + (-b)</code> \\(a / b\\) <code>a * b^{-1}</code> <p>Note</p> <p>No new gradient logic needed! They just reuse <code>__add__</code>, <code>__mul__</code>, and <code>__pow__</code>.</p> <p>The <code>__radd__</code> and <code>__rmul__</code> variants handle cases like <code>3 + value</code> (when the <code>Value</code> is on the right side of the operator).</p>"},{"location":"02-calculus-and-autograd/02-the-value-class/#the-computation-graph-so-far","title":"The Computation Graph So Far","text":"<p>Every operation creates a new <code>Value</code> node, linked to its children:</p> <pre><code>a = Value(2.0)\nb = Value(3.0)\nc = a + b        # 5.0\nd = c * a        # 10.0\ne = d.log()      # 2.302...\n</code></pre> <pre><code>flowchart TD\n    A[\"a (2.0)\"] --&gt; C[\"c = a+b (5.0)\"]\n    B[\"b (3.0)\"] --&gt; C\n    C --&gt; D[\"d = c\u00d7a (10.0)\"]\n    A --&gt; D\n    D --&gt; E[\"e = ln(d) (2.302)\"]\n\n    style A fill:#1de9b6,stroke:#0db99a,color:#fff\n    style B fill:#1de9b6,stroke:#0db99a,color:#fff\n    style C fill:#12a889,stroke:#0a7d68,color:#fff\n    style D fill:#0d8872,stroke:#085c4e,color:#fff\n    style E fill:#1de9b6,stroke:#0db99a,color:#000</code></pre> <p>The graph records the entire computation. Now we need to walk it backwards to compute gradients.</p> Terminology Term Meaning Value A wrapper around a number that tracks how it was computed Computation graph The tree of <code>Value</code> nodes showing all operations Local gradient The derivative of one operation w.r.t. its inputs Autograd Automatic differentiation \u2014 computing all gradients automatically ReLU \\(\\max(0, x)\\) \u2014 an activation function that zeros out negatives"},{"location":"02-calculus-and-autograd/03-forward-pass/","title":"The Forward Pass","text":""},{"location":"02-calculus-and-autograd/03-forward-pass/#what-is-a-forward-pass","title":"What Is a \"Forward Pass\"?","text":"<p>The forward pass is simply computing the output given an input. You feed in numbers on one end, and math operations transform them step by step until you get a result.</p> <p>It's called \"forward\" because data flows in one direction: from inputs \u2192 through operations \u2192 to output.</p>"},{"location":"02-calculus-and-autograd/03-forward-pass/#a-concrete-example","title":"A Concrete Example","text":"<p>Let's trace a tiny computation:</p> <pre><code>a = Value(2.0)\nb = Value(-3.0)\nc = a * b        # -6.0\nd = Value(10.0)\ne = c + d        # 4.0\nf = e.relu()     # 4.0 (positive, so unchanged)\n</code></pre> Step-by-stepAs a graph Step Computation Result 1 <code>a.data = 2.0</code> given 2 <code>b.data = -3.0</code> given 3 <code>c.data = 2.0 \u00d7 (-3.0)</code> -6.0 4 <code>d.data = 10.0</code> given 5 <code>e.data = -6.0 + 10.0</code> 4.0 6 <code>f.data = max(0, 4.0)</code> 4.0 <pre><code>flowchart LR\n    A[\"a (2.0)\"] --&gt; MUL[\"\u00d7 \u2192 c (-6.0)\"]\n    B[\"b (-3.0)\"] --&gt; MUL\n    MUL --&gt; ADD[\"+ \u2192 e (4.0)\"]\n    D[\"d (10.0)\"] --&gt; ADD\n    ADD --&gt; RELU[\"relu \u2192 f (4.0)\"]\n\n    style A fill:#1de9b6,stroke:#0db99a,color:#fff\n    style B fill:#1de9b6,stroke:#0db99a,color:#fff\n    style D fill:#1de9b6,stroke:#0db99a,color:#fff\n    style MUL fill:#12a889,stroke:#0a7d68,color:#fff\n    style ADD fill:#0d8872,stroke:#085c4e,color:#fff\n    style RELU fill:#1de9b6,stroke:#0db99a,color:#000</code></pre> <p>Each arrow is a <code>Value</code> node. Each operation creates a new node.</p>"},{"location":"02-calculus-and-autograd/03-forward-pass/#what-gets-recorded","title":"What Gets Recorded","text":"<p>During the forward pass, each new <code>Value</code> stores:</p> <ol> <li>The computed result (<code>.data</code>)</li> <li>References to the inputs (<code>._children</code>)</li> <li>The local derivatives (<code>._local_grads</code>)</li> </ol> <p>For node <code>c = a * b</code></p> <pre><code>c.data = -6.0\nc._children = (a, b)\nc._local_grads = (-3.0, 2.0)   # (b.data, a.data)\n</code></pre> <p>For node <code>e = c + d</code></p> <pre><code>e.data = 4.0\ne._children = (c, d)\ne._local_grads = (1, 1)\n</code></pre> <p>This recording is building the computation graph as a side effect of the forward pass. We'll need this graph for the backward pass.</p>"},{"location":"02-calculus-and-autograd/03-forward-pass/#the-forward-pass-in-microgptpy","title":"The Forward Pass in microgpt.py","text":"<p>In the actual model, the forward pass happens when we call the <code>gpt()</code> function:</p> microgpt.py \u2014 Line 165<pre><code>logits = gpt(token_id, pos_id, keys, values)\n</code></pre> <p>This call triggers a cascade of hundreds of operations:</p> <ol> <li>Look up embeddings (addition of two <code>Value</code> rows)</li> <li>Normalize (multiply, divide, power operations on <code>Value</code> nodes)</li> <li>Attention (matrix multiplications, softmax \u2014 all on <code>Value</code> nodes)</li> <li>MLP (more linear transforms and activation)</li> <li>Output logits (one final linear transform)</li> </ol> <p>Important</p> <p>Every single arithmetic operation creates a new <code>Value</code> node, and by the time <code>logits</code> is returned, there's a massive computation graph in memory, with every node remembering exactly how it was produced.</p>"},{"location":"02-calculus-and-autograd/03-forward-pass/#why-build-this-graph","title":"Why Build This Graph?","text":"<p>Because the backward pass will walk this graph in reverse to compute gradients. Without the graph, we wouldn't know which operations happened, in what order, with what inputs. The graph is the \"recording\" that makes automatic differentiation possible.</p>"},{"location":"02-calculus-and-autograd/03-forward-pass/#the-full-picture","title":"The Full Picture","text":"<pre><code>flowchart LR\n    direction LR\n\n    subgraph inputs[\"Inputs\"]\n        A[\"a (2.0)\"]\n        B[\"b (-3.0)\"]\n        D[\"d (10.0)\"]\n    end\n\n    subgraph ops[\"Operations\"]\n        MUL[\"\u00d7 \u2192 c (-6.0)\"]\n        ADD[\"+ \u2192 e (4.0)\"]\n        RELU[\"relu \u2192 f (4.0)\"]\n    end\n\n    A --&gt; MUL\n    B --&gt; MUL\n    MUL --&gt; ADD\n    D --&gt; ADD\n    ADD --&gt; RELU\n\n    style inputs fill:none,stroke:#1de9b6\n    style ops fill:none,stroke:#1de9b6</code></pre> <p>FORWARD = left to right (compute values) \u27a1\ufe0f</p> <p>BACKWARD = right to left (compute gradients) \u2b05\ufe0f</p> Terminology Term Meaning Forward pass Computing the output from the input, step by step Computation graph The tree of <code>Value</code> nodes built during the forward pass Leaf node An input <code>Value</code> with no children (parameters, inputs) Internal node A <code>Value</code> created by an operation on other <code>Value</code>s Root node The final output (usually the loss)"},{"location":"02-calculus-and-autograd/04-backward-pass/","title":"The Backward Pass","text":""},{"location":"02-calculus-and-autograd/04-backward-pass/#the-problem","title":"The Problem","text":"<p>After the forward pass, we have:</p> <ul> <li>A final output: the loss (a single number measuring how wrong the model was)</li> <li>A computation graph: every operation that led to the loss, recorded as <code>Value</code> nodes</li> </ul> <p>Now we need to answer: for every parameter in the model, how much did it contribute to the loss? That is, we need \\(\\frac{d(\\text{loss})}{d(\\text{parameter})}\\) for every parameter.</p>"},{"location":"02-calculus-and-autograd/04-backward-pass/#the-backward-pass-algorithm-lines-5972","title":"The Backward Pass Algorithm (Lines 59\u201372)","text":"microgpt.py \u2014 Lines 59-72<pre><code>def backward(self):\n    topo = []\n    visited = set()\n    def build_topo(v):\n        if v not in visited:\n            visited.add(v)\n            for child in v._children:\n                build_topo(child)\n            topo.append(v)\n    build_topo(self)\n    self.grad = 1\n    for v in reversed(topo):\n        for child, local_grad in zip(v._children, v._local_grads):\n            child.grad += local_grad * v.grad\n</code></pre> <p>This is only 14 lines of code, but it's doing something profound. Let's decompose it.</p>"},{"location":"02-calculus-and-autograd/04-backward-pass/#step-1-topological-sort-lines-6068","title":"Step 1: Topological Sort (Lines 60\u201368)","text":"microgpt.py \u2014 Lines 60-68<pre><code>topo = []\nvisited = set()\ndef build_topo(v):\n    if v not in visited:\n        visited.add(v)\n        for child in v._children:\n            build_topo(child)\n        topo.append(v)\nbuild_topo(self)\n</code></pre> <p>What is a topological sort?</p> <p>A topological sort is an ordering of nodes such that every node comes after all its children.</p> <p>The algorithm uses depth-first search: visit all children before adding yourself. The <code>visited</code> set prevents visiting the same node twice (since a node can be used in multiple places).</p>"},{"location":"02-calculus-and-autograd/04-backward-pass/#why-do-we-need-this","title":"Why do we need this?","text":"<p>Because the backward pass processes nodes from the output back to the inputs. By reversing the topological order (<code>reversed(topo)</code>), we guarantee that every node's gradient is fully computed before we try to propagate it to its children.</p>"},{"location":"02-calculus-and-autograd/04-backward-pass/#step-2-seed-the-gradient-line-69","title":"Step 2: Seed the Gradient (Line 69)","text":"microgpt.py \u2014 Line 69<pre><code>self.grad = 1\n</code></pre> <p><code>self</code> is the loss node \u2014 the root of the graph. We set its gradient to 1 because:</p> \\[\\frac{d(\\text{loss})}{d(\\text{loss})} = 1\\] <p>The derivative of anything with respect to itself is always 1. This is our starting point.</p>"},{"location":"02-calculus-and-autograd/04-backward-pass/#step-3-propagate-gradients-backward-lines-7072","title":"Step 3: Propagate Gradients Backward (Lines 70\u201372)","text":"microgpt.py \u2014 Lines 70-72<pre><code>for v in reversed(topo):\n    for child, local_grad in zip(v._children, v._local_grads):\n        child.grad += local_grad * v.grad\n</code></pre> <p>For each node \\(v\\) (starting from the loss, going backwards):</p> \\[\\text{child.grad} \\mathrel{+}= \\text{local\\_grad} \\times v\\text{.grad}\\] <p>This is the chain rule in action: the gradient of a child = (local gradient) \u00d7 (parent's gradient). We use <code>+=</code> because a node might have multiple parents.</p>"},{"location":"02-calculus-and-autograd/04-backward-pass/#full-walkthrough","title":"Full Walkthrough","text":"<p>Let's trace the backward pass for \\(f = \\text{relu}(a \\times b + d)\\):</p> Step 1: SeedStep 2: Process reluStep 3: Process additionStep 4: Process multiplication \\[f\\text{.grad} = 1 \\quad \\text{(starting point)}\\] <p>\\(f = \\text{relu}(e)\\), local grad at \\(e = 4.0\\) is \\(1.0\\) (positive input)</p> \\[e\\text{.grad} \\mathrel{+}= 1.0 \\times f\\text{.grad} = 1.0 \\times 1 = 1.0\\] <p>\\(e = c + d\\), local grads are both \\(1\\)</p> \\[c\\text{.grad} \\mathrel{+}= 1 \\times e\\text{.grad} = 1 \\times 1.0 = 1.0\\] \\[d\\text{.grad} \\mathrel{+}= 1 \\times e\\text{.grad} = 1 \\times 1.0 = 1.0\\] <p>\\(c = a \\times b\\), local grad w.r.t. \\(a\\) is \\(b = -3.0\\), w.r.t. \\(b\\) is \\(a = 2.0\\)</p> \\[a\\text{.grad} \\mathrel{+}= (-3.0) \\times c\\text{.grad} = -3.0 \\times 1.0 = -3.0\\] \\[b\\text{.grad} \\mathrel{+}= 2.0 \\times c\\text{.grad} = 2.0 \\times 1.0 = 2.0\\] <p>Result:</p> Node <code>.grad</code> Meaning \\(a\\) \\(-3.0\\) \"If \\(a\\) increases by 1, \\(f\\) decreases by 3\" \\(b\\) \\(2.0\\) \"If \\(b\\) increases by 1, \\(f\\) increases by 2\" \\(c\\) \\(1.0\\) \\(d\\) \\(1.0\\) \\(e\\) \\(1.0\\) \\(f\\) \\(1.0\\) seed <p>Verification</p> \\[f(a=2.0) = \\text{relu}(2.0 \\times (-3.0) + 10.0) = \\text{relu}(4.0) = 4.0\\] \\[f(a=2.001) = \\text{relu}(2.001 \\times (-3.0) + 10.0) = \\text{relu}(3.997) = 3.997\\] \\[\\frac{\\Delta f}{\\Delta a} = \\frac{3.997 - 4.0}{0.001} = -3.0 \\checkmark\\]"},{"location":"02-calculus-and-autograd/04-backward-pass/#the-is-crucial","title":"The \"+=\" is Crucial","text":"<p>Warning</p> <p>Notice <code>child.grad +=</code> (not <code>=</code>). This is because a value might be used in multiple places:</p> <pre><code>a = Value(3.0)\nb = a + a       # a is used twice!\n</code></pre> <p>Here \\(a\\) is a child of \\(b\\) through two paths. The gradient contributions from both paths must be summed:</p> \\[\\frac{db}{da} = 1 + 1 = 2\\] <p>And indeed, if \\(a = 3\\) then \\(b = 6\\), and if \\(a = 4\\) then \\(b = 8\\). Rate of change = 2. </p>"},{"location":"02-calculus-and-autograd/04-backward-pass/#what-this-means-for-training","title":"What This Means for Training","text":"<p>After calling <code>loss.backward()</code>:</p> <ul> <li>Every parameter (<code>Value</code> in the model) has its <code>.grad</code> set</li> <li>This gradient tells us: \"nudge this parameter in the opposite direction of <code>.grad</code> to reduce the loss\"</li> <li>The optimizer then uses these gradients to update all parameters</li> </ul> <p>Important</p> <p>All of this happens automatically. The programmer only needs to:</p> <ol> <li>Build the forward pass (which records the graph)</li> <li>Call <code>.backward()</code> on the loss</li> </ol> Terminology Term Meaning Backward pass Walking the graph in reverse to compute gradients Topological sort Ordering nodes so children come before parents Gradient accumulation Summing gradient contributions from multiple paths (<code>+=</code>) Seed gradient Setting the loss's gradient to 1 to start the backward pass Backpropagation Another name for the backward pass"},{"location":"02-calculus-and-autograd/05-building-a-computation-graph/","title":"Building a Computation Graph","text":""},{"location":"02-calculus-and-autograd/05-building-a-computation-graph/#putting-it-all-together","title":"Putting It All Together","text":"<p>We've seen the individual pieces:</p> <ul> <li><code>Value</code> wraps numbers and records operations</li> <li>The forward pass builds the graph</li> <li>The backward pass walks it in reverse to compute gradients</li> </ul> <p>Now let's see how a realistic example looks \u2014 one that mirrors what actually happens inside <code>microgpt.py</code>.</p>"},{"location":"02-calculus-and-autograd/05-building-a-computation-graph/#a-mini-neural-network","title":"A Mini Neural Network","text":"<p>Let's build the simplest possible \"network\" \u2014 one that takes a number and tries to predict another number:</p> <pre><code># Two parameters (the model's \"knowledge\")\nw = Value(0.5)    # weight\nb = Value(0.1)    # bias\n\n# Input and target\nx = 2.0           # input (plain number, not a Value)\ntarget = 3.0      # we want the model to output 3.0\n\n# Forward pass: the prediction\nprediction = w * x + b    # 0.5 \u00d7 2.0 + 0.1 = 1.1\n\n# Loss: how wrong are we?\nerror = prediction - target    # 1.1 - 3.0 = -1.9\nloss = error ** 2              # (-1.9)\u00b2 = 3.61\n</code></pre>"},{"location":"02-calculus-and-autograd/05-building-a-computation-graph/#the-graph-that-gets-built","title":"The graph that gets built:","text":"<pre><code>flowchart LR\n    W[\"w (0.5)\"] --&gt; MUL[\"\u00d7 \u2192 wx (1.0)\"]\n    X[\"x (2.0)\"] --&gt; MUL\n    MUL --&gt; ADD[\"+ \u2192 pred (1.1)\"]\n    B[\"b (0.1)\"] --&gt; ADD\n    ADD --&gt; SUB[\"- \u2192 err (-1.9)\"]\n    T[\"target (3.0)\"] --&gt; SUB\n    SUB --&gt; POW[\"\u00b2 \u2192 loss (3.61)\"]\n\n    style W fill:#1de9b6,stroke:#0db99a,color:#fff\n    style B fill:#1de9b6,stroke:#0db99a,color:#fff\n    style X fill:#12a889,stroke:#0a7d68,color:#fff\n    style T fill:#12a889,stroke:#0a7d68,color:#fff\n    style POW fill:#64ffda,stroke:#4dd4b0,color:#fff</code></pre>"},{"location":"02-calculus-and-autograd/05-building-a-computation-graph/#now-backward","title":"Now backward:","text":"<pre><code>loss.backward()\n</code></pre> Gradient computationWhat the gradients meanUpdate parameters Node Gradient Computation <code>loss</code> \\(1.0\\) seed <code>err</code> \\(-3.8\\) \\(2 \\times (-1.9) \\times 1.0\\) (power rule) <code>pred</code> \\(-3.8\\) \\(1 \\times \\text{err.grad}\\) <code>b</code> \\(-3.8\\) \\(1 \\times \\text{pred.grad}\\) (addition) <code>wx</code> \\(-3.8\\) \\(1 \\times \\text{pred.grad}\\) (addition) <code>w</code> \\(-7.6\\) \\(x \\times \\text{wx.grad} = 2.0 \\times (-3.8)\\) \\[w\\text{.grad} = -7.6 \\implies \\text{increasing } w \\text{ would DECREASE the loss}\\] \\[b\\text{.grad} = -3.8 \\implies \\text{increasing } b \\text{ would DECREASE the loss}\\] <p>This makes sense! Our prediction was 1.1 but the target was 3.0 \u2014 we're too low. Both \\(w\\) and \\(b\\) need to increase.</p> <pre><code>learning_rate = 0.01\nw.data -= learning_rate * w.grad   # 0.5 - 0.01\u00d7(-7.6) = 0.576\nb.data -= learning_rate * b.grad   # 0.1 - 0.01\u00d7(-3.8) = 0.138\n</code></pre> <p>New prediction: \\(0.576 \\times 2.0 + 0.138 = 1.29\\) (closer to 3.0!)</p> <p>Repeat this hundreds of times, and \\(w\\) and \\(b\\) will converge to values that make the prediction close to 3.0.</p>"},{"location":"02-calculus-and-autograd/05-building-a-computation-graph/#scale-what-the-real-graph-looks-like","title":"Scale: What the Real Graph Looks Like","text":"<p>In the mini example above, the graph had ~8 nodes. In <code>microgpt.py</code>, a single forward pass through the <code>gpt()</code> function creates thousands of nodes:</p> Component Approximate # of Value operations Embedding lookup ~16 RMSNorm ~50 One attention head ~200 Four attention heads ~800 MLP (expand + activate + compress) ~1,500 Output linear ~400 Softmax ~80 Loss (log) ~2 Total per token ~3,000+ Per 8-token sequence ~24,000+ <p>The Power of Autograd</p> <p>All of these nodes are <code>Value</code> objects sitting in memory, linked together. When you call <code>loss.backward()</code>, the algorithm visits each node exactly once (thanks to topological sort) and computes the gradient. One pass, all gradients, every parameter.</p> <ul> <li>Without autograd: ~20,000 forward passes to estimate gradients for 10,000 parameters</li> <li>With autograd: Exactly 2 passes \u2014 one forward, one backward</li> </ul>"},{"location":"02-calculus-and-autograd/05-building-a-computation-graph/#checkpoint","title":"Checkpoint \u2713","text":"<p>What you understand now</p> <ul> <li> Derivatives: \"which direction to nudge\" (Lesson 0)</li> <li> Chain rule: composing derivatives through a chain (Lesson 1)</li> <li> Value class: recording operations and storing local gradients (Lesson 2)</li> <li> Forward pass: computing the output and building the graph (Lesson 3)</li> <li> Backward pass: walking the graph in reverse to compute all gradients (Lesson 4)</li> <li> The full picture: how these pieces work together (this lesson)</li> </ul> <p>What we don't know yet: what does the <code>gpt()</code> function actually compute? What is attention? What is a linear layer? For that, we need to understand the architecture.</p>"},{"location":"03-the-architecture/00-parameters-are-knowledge/","title":"Parameters Are Knowledge","text":""},{"location":"03-the-architecture/00-parameters-are-knowledge/#the-problem","title":"The Problem","text":"<p>We have an autograd engine that can compute gradients. But gradients of what? We need actual numbers to compute with \u2014 the model's parameters.</p> <p>Parameters are the thousands of numbers that the model will tune during training to get good at prediction. Before training, they're random. After training, they encode everything the model has \"learned.\"</p>"},{"location":"03-the-architecture/00-parameters-are-knowledge/#the-hyperparameters-lines-7579","title":"The Hyperparameters (Lines 75\u201379)","text":"microgpt.py \u2014 Lines 75-79<pre><code>n_embd = 16     # embedding dimension\nn_head = 4      # number of attention heads\nn_layer = 1     # number of layers\nblock_size = 8  # maximum sequence length\nhead_dim = n_embd // n_head  # dimension of each head = 4\n</code></pre> <p>These are hyperparameters \u2014 settings that the programmer chooses, not things the model learns:</p> Hyperparameter Value What it controls <code>n_embd</code> 16 How \"rich\" each token's representation is <code>n_head</code> 4 How many different \"perspectives\" in attention <code>n_layer</code> 1 How many times we repeat the attention+MLP block <code>block_size</code> 8 Maximum number of characters the model can see <code>head_dim</code> 4 Size of each attention head (\\(16 / 4 = 4\\)) <p>Scale comparison</p> <p>In real GPT models, these numbers are much larger (GPT-2: <code>n_embd=768, n_head=12, n_layer=12</code>). The structure is identical.</p>"},{"location":"03-the-architecture/00-parameters-are-knowledge/#creating-parameter-matrices-line-80","title":"Creating Parameter Matrices (Line 80)","text":"microgpt.py \u2014 Line 80<pre><code>matrix = lambda nout, nin, std=0.02: [[Value(random.gauss(0, std)) for _ in range(nin)] for _ in range(nout)]\n</code></pre> <p>This helper creates a 2D grid (matrix) of <code>Value</code> objects, each initialized with a small random number from a Gaussian distribution:</p> \\[\\text{random.gauss}(0, 0.02) \\implies \\text{a random number near 0, usually between } -0.06 \\text{ and } +0.06\\] <p>Example: <code>matrix(3, 2)</code></p> <pre><code>[\n  [Value(0.01), Value(-0.03)],   # row 0\n  [Value(0.02), Value(0.01)],    # row 1\n  [Value(-0.01), Value(0.04)],   # row 2\n]\n</code></pre> <p>A 3\u00d72 grid of random <code>Value</code> objects.</p> Why random?Why small (std=0.02)?Why Gaussian? <p>If all parameters started at the same value, they'd all receive the same gradient and update in lockstep forever. Randomness breaks this symmetry.</p> <p>Large initial values cause numerical instability. Starting near zero is safe.</p> <p>Draws values from a bell curve centered at 0. Most values are close to 0, rarely far from it.</p>"},{"location":"03-the-architecture/00-parameters-are-knowledge/#the-state-dictionary-lines-8189","title":"The State Dictionary (Lines 81\u201389)","text":"microgpt.py \u2014 Lines 81-89<pre><code>state_dict = {\n    'wte': matrix(vocab_size, n_embd),   # token embeddings: 27 \u00d7 16\n    'wpe': matrix(block_size, n_embd),    # position embeddings: 8 \u00d7 16\n    'lm_head': matrix(vocab_size, n_embd), # output layer: 27 \u00d7 16\n}\nfor i in range(n_layer):\n    state_dict[f'layer{i}.attn_wq'] = matrix(n_embd, n_embd)    # 16 \u00d7 16\n    state_dict[f'layer{i}.attn_wk'] = matrix(n_embd, n_embd)    # 16 \u00d7 16\n    state_dict[f'layer{i}.attn_wv'] = matrix(n_embd, n_embd)    # 16 \u00d7 16\n    state_dict[f'layer{i}.attn_wo'] = matrix(n_embd, n_embd, std=0)  # 16 \u00d7 16\n    state_dict[f'layer{i}.mlp_fc1'] = matrix(4 * n_embd, n_embd)  # 64 \u00d7 16\n    state_dict[f'layer{i}.mlp_fc2'] = matrix(n_embd, 4 * n_embd, std=0)  # 16 \u00d7 64\n</code></pre> Name Shape Purpose <code>wte</code> 27 \u00d7 16 Token embedding \u2014 gives each of 27 tokens a 16-dimensional \"meaning\" <code>wpe</code> 8 \u00d7 16 Position embedding \u2014 encodes position (1st, 2nd, ..., 8th) <code>lm_head</code> 27 \u00d7 16 Output layer \u2014 converts internal state back to token predictions <code>attn_wq</code> 16 \u00d7 16 Query weights for attention <code>attn_wk</code> 16 \u00d7 16 Key weights for attention <code>attn_wv</code> 16 \u00d7 16 Value weights for attention (not <code>Value</code> class \u2014 confusing, but standard terminology) <code>attn_wo</code> 16 \u00d7 16 Output projection for attention <code>mlp_fc1</code> 64 \u00d7 16 Expand layer in the MLP block (16 \u2192 64) <code>mlp_fc2</code> 16 \u00d7 64 Compress layer in the MLP block (64 \u2192 16) <p>Why <code>std=0</code> for some matrices?</p> <p><code>attn_wo</code> and <code>mlp_fc2</code> are initialized with <code>std=0</code> \u2014 all zeros. These are output projection matrices. Initializing them to zero means the attention and MLP blocks initially do nothing (they output zeros, so the residual connection just passes the input through). This is a stability trick for training.</p>"},{"location":"03-the-architecture/00-parameters-are-knowledge/#flattening-the-parameters-line-89","title":"Flattening the Parameters (Line 89)","text":"microgpt.py \u2014 Line 89<pre><code>params = [p for mat in state_dict.values() for row in mat for p in row]\nprint(f\"num params: {len(params)}\")\n</code></pre> <p>This flattens all matrices into a single flat list of <code>Value</code> objects. The optimizer needs one flat list to loop over all parameters.</p>"},{"location":"03-the-architecture/00-parameters-are-knowledge/#how-many-parameters","title":"How many parameters?","text":"Matrix Shape Count <code>wte</code> 27 \u00d7 16 432 <code>wpe</code> 8 \u00d7 16 128 <code>lm_head</code> 27 \u00d7 16 432 <code>attn_wq</code> 16 \u00d7 16 256 <code>attn_wk</code> 16 \u00d7 16 256 <code>attn_wv</code> 16 \u00d7 16 256 <code>attn_wo</code> 16 \u00d7 16 256 <code>mlp_fc1</code> 64 \u00d7 16 1,024 <code>mlp_fc2</code> 16 \u00d7 64 1,024 Total 4,064 <p>4,064 <code>Value</code> objects, each a small random number, each tracking its gradient. By comparison, GPT-2 has 124 million parameters, and GPT-4 is rumored to have over a trillion.</p> Terminology Term Meaning Parameters The learnable numbers in the model (weights and biases) Hyperparameters Settings chosen by the programmer (<code>n_embd</code>, <code>n_head</code>, etc.) State dict A dictionary mapping names to parameter matrices Weight matrix A 2D grid of parameters used in a linear transformation Initialization The strategy for setting initial parameter values Gaussian A bell-curve distribution; most values cluster near the mean"},{"location":"03-the-architecture/01-embeddings/","title":"Embeddings","text":""},{"location":"03-the-architecture/01-embeddings/#the-problem","title":"The Problem","text":"<p>We have token IDs: <code>'e'</code> = 4, <code>'m'</code> = 12, etc. But a single number doesn't carry much information. The model needs a richer representation.</p> <p>Consider: is <code>'e'</code> (4) more similar to <code>'d'</code> (3) than to <code>'z'</code> (25)? No \u2014 the numeric ordering is arbitrary. But with a single number, the model has no way to tell.</p>"},{"location":"03-the-architecture/01-embeddings/#the-solution-embedding-vectors","title":"The Solution: Embedding Vectors","text":"<p>Instead of representing each token as a single number, we represent it as a list of numbers (a vector). Each token gets its own 16-dimensional vector:</p> <pre><code>'a' (token 0)  \u2192 [0.01, -0.03, 0.02, ..., 0.04]   (16 numbers)\n'b' (token 1)  \u2192 [-0.02, 0.01, -0.01, ..., 0.03]   (16 numbers)\n'e' (token 4)  \u2192 [0.03, 0.02, -0.04, ..., -0.01]   (16 numbers)\n</code></pre> <p>These 16 numbers capture a token's \"meaning\" in a way the model can work with. After training, tokens with similar roles will have similar vectors.</p>"},{"location":"03-the-architecture/01-embeddings/#the-code-lines-109111","title":"The Code (Lines 109\u2013111)","text":"microgpt.py \u2014 Lines 109-111 (inside gpt())<pre><code>tok_emb = state_dict['wte'][token_id]  # token embedding\npos_emb = state_dict['wpe'][pos_id]    # position embedding\nx = [t + p for t, p in zip(tok_emb, pos_emb)]  # combined embedding\n</code></pre> Line 109: Token EmbeddingLine 110: Position EmbeddingLine 111: Combine <pre><code>tok_emb = state_dict['wte'][token_id]\n</code></pre> <p><code>state_dict['wte']</code> is a 27\u00d716 matrix \u2014 the token embedding table. For <code>token_id = 4</code> (the letter <code>'e'</code>), we grab row 4 \u2014 a list of 16 <code>Value</code> objects.</p> <p>This is not a computation, just a lookup. But the values in this table are parameters \u2014 they'll be updated during training.</p> <pre><code>pos_emb = state_dict['wpe'][pos_id]\n</code></pre> <p><code>state_dict['wpe']</code> is an 8\u00d716 matrix \u2014 the position embedding table. It encodes where a token sits in the sequence.</p> <p>Why positions matter</p> <p>The letter <code>'e'</code> at position 0 (first letter of a name) is very different from <code>'e'</code> at position 4 (middle of a name). Without position information, the model would treat every occurrence identically.</p> <pre><code>x = [t + p for t, p in zip(tok_emb, pos_emb)]\n</code></pre> <p>Add token and position embeddings element-wise:</p> \\[x_i = \\text{tok\\_emb}_i + \\text{pos\\_emb}_i \\quad \\text{(\"what\" + \"where\")}\\] <p>Why addition (not concatenation)? Simpler and works well in practice. Both embeddings live in the same 16-dimensional space.</p>"},{"location":"03-the-architecture/01-embeddings/#visual-summary","title":"Visual Summary","text":"<pre><code>flowchart LR\n    TID[\"token_id = 4\\n('e')\"] --&gt; WTE[\"wte\\n27 \u00d7 16\"]\n    WTE --&gt; |\"row 4\"| TOK[\"tok_emb\\n(16 Values)\"]\n    PID[\"pos_id = 2\"] --&gt; WPE[\"wpe\\n8 \u00d7 16\"]\n    WPE --&gt; |\"row 2\"| POS[\"pos_emb\\n(16 Values)\"]\n    TOK --&gt; ADD[\"\u2295 element-wise add\"]\n    POS --&gt; ADD\n    ADD --&gt; X[\"x = 'e' at position 2\\n(16 Values)\"]\n\n    style TID fill:#1de9b6,stroke:#0db99a,color:#fff\n    style PID fill:#1de9b6,stroke:#0db99a,color:#fff\n    style X fill:#1de9b6,stroke:#0db99a,color:#000</code></pre>"},{"location":"03-the-architecture/01-embeddings/#why-16-dimensions","title":"Why 16 Dimensions?","text":"<p>The choice of 16 is a hyperparameter (<code>n_embd</code>). More dimensions = more expressive power, but also more parameters and slower computation. For our tiny names dataset, 16 is sufficient.</p> <p>Info</p> <p>In GPT-2, <code>n_embd = 768</code>. Each token is a 768-dimensional vector. That's a much richer representation, needed for understanding complex language.</p> Terminology Term Meaning Embedding A vector (list of numbers) representing a token Embedding table A matrix where each row is one token's embedding Token embedding Encodes what the token is Position embedding Encodes where the token is in the sequence Lookup Selecting a row from a table by index (no arithmetic) Dimension The number of elements in an embedding vector"},{"location":"03-the-architecture/02-linear-layers/","title":"Linear Layers","text":""},{"location":"03-the-architecture/02-linear-layers/#the-problem","title":"The Problem","text":"<p>We have a 16-dimensional embedding vector <code>x</code>. Now we need to transform it \u2014 mix the information in its 16 dimensions to produce a new vector. This is the most fundamental computation in neural networks.</p>"},{"location":"03-the-architecture/02-linear-layers/#what-is-a-linear-layer","title":"What Is a Linear Layer?","text":"<p>A linear layer multiplies an input vector by a weight matrix. It's the neural network's way of \"mixing and recombining\" information.</p>"},{"location":"03-the-architecture/02-linear-layers/#concrete-example","title":"Concrete Example","text":"<p>A tiny 3\u21922 example (3 inputs, 2 outputs):</p> \\[\\mathbf{x} = [x_0, x_1, x_2], \\quad W = \\begin{bmatrix} w_{00} &amp; w_{01} &amp; w_{02} \\\\ w_{10} &amp; w_{11} &amp; w_{12} \\end{bmatrix}\\] \\[y_0 = w_{00} \\cdot x_0 + w_{01} \\cdot x_1 + w_{02} \\cdot x_2\\] \\[y_1 = w_{10} \\cdot x_0 + w_{11} \\cdot x_1 + w_{12} \\cdot x_2\\] <p>Each output element is a weighted sum of all input elements. The weights determine \"how much of each input goes into each output.\"</p> <p>With numbers</p> \\[\\mathbf{x} = [2.0, 3.0, 1.0]\\] \\[W = \\begin{bmatrix} 0.5 &amp; -0.3 &amp; 0.1 \\\\ 0.2 &amp; 0.4 &amp; -0.2 \\end{bmatrix}\\] \\[y_0 = 0.5 \\times 2 + (-0.3) \\times 3 + 0.1 \\times 1 = 1.0 - 0.9 + 0.1 = 0.2\\] \\[y_1 = 0.2 \\times 2 + 0.4 \\times 3 + (-0.2) \\times 1 = 0.4 + 1.2 - 0.2 = 1.4\\] \\[\\mathbf{y} = [0.2, 1.4]\\]"},{"location":"03-the-architecture/02-linear-layers/#the-code-lines-9495","title":"The Code (Lines 94\u201395)","text":"microgpt.py \u2014 Lines 94-95<pre><code>def linear(x, w):\n    return [sum(wi * xi for wi, xi in zip(wo, x)) for wo in w]\n</code></pre> UnpackedTraced with numbers <pre><code># For each row 'wo' in weight matrix 'w':\nfor wo in w:\n    # Compute the dot product of that row with input 'x':\n    sum(wi * xi for wi, xi in zip(wo, x))\n</code></pre> <pre><code>Row 0: wo = [0.5, -0.3, 0.1]\n       sum(0.5\u00d72, -0.3\u00d73, 0.1\u00d71) = 0.2\n\nRow 1: wo = [0.2, 0.4, -0.2]\n       sum(0.2\u00d72, 0.4\u00d73, -0.2\u00d71) = 1.4\n\nResult: [0.2, 1.4]\n</code></pre>"},{"location":"03-the-architecture/02-linear-layers/#whats-a-dot-product","title":"What's a Dot Product?","text":"<p>The inner sum is called a dot product:</p> \\[\\text{dot}([a, b, c], [d, e, f]) = a \\cdot d + b \\cdot e + c \\cdot f\\] <p>It's a measure of \"similarity\" between two vectors. High dot product = vectors point in the same direction.</p>"},{"location":"03-the-architecture/02-linear-layers/#visual-representation","title":"Visual Representation","text":"<pre><code>flowchart LR\n    X[\"x\\n[x\u2080, x\u2081, x\u2082]\"] --&gt; ROW0[\"row 0: [w\u2080\u2080, w\u2080\u2081, w\u2080\u2082]\\n\u2192 dot product \u2192 y\u2080\"]\n    X --&gt; ROW1[\"row 1: [w\u2081\u2080, w\u2081\u2081, w\u2081\u2082]\\n\u2192 dot product \u2192 y\u2081\"]\n\n    style X fill:#1de9b6,stroke:#0db99a,color:#fff\n    style ROW0 fill:#1de9b6,stroke:#0db99a,color:#000\n    style ROW1 fill:#1de9b6,stroke:#0db99a,color:#000</code></pre> <p>Each row of the weight matrix produces one output element. The number of rows determines the output size.</p>"},{"location":"03-the-architecture/02-linear-layers/#where-linear-layers-appear-in-microgptpy","title":"Where Linear Layers Appear in microgpt.py","text":"Line Usage Transform 118 <code>linear(x, attn_wq)</code> 16 \u2192 16 (queries) 119 <code>linear(x, attn_wk)</code> 16 \u2192 16 (keys) 120 <code>linear(x, attn_wv)</code> 16 \u2192 16 (values) 133 <code>linear(x_attn, attn_wo)</code> 16 \u2192 16 (output projection) 138 <code>linear(x, mlp_fc1)</code> 16 \u2192 64 (expand) 140 <code>linear(x, mlp_fc2)</code> 64 \u2192 16 (compress) 143 <code>linear(x, lm_head)</code> 16 \u2192 27 (final prediction) <p>The <code>linear</code> function is the workhorse \u2014 used 7 times per layer, plus one more for the output.</p>"},{"location":"03-the-architecture/02-linear-layers/#why-linear","title":"Why \"Linear\"?","text":"<p>Because \\(y = Wx\\) is a linear function \u2014 if you double the input, you double the output. No curves, no bends. Just scaling and mixing.</p> <p>Limitation</p> <p>Linear layers can only represent straight-line relationships. To model complex patterns, we need non-linearity \u2014 that's what activation functions like ReLU provide.</p> <p>No Bias Terms</p> <p>Standard neural networks often add a bias: \\(y = Wx + b\\). Karpathy's implementation skips biases entirely \u2014 a simplification common in modern Transformer architectures.</p> Terminology Term Meaning Linear layer Multiplying input by a weight matrix; \\(y = Wx\\) Dot product Multiply elements pairwise, then sum Weight matrix The grid of learnable parameters in a linear layer Bias An optional additive vector (omitted in microgpt.py) Projection Another word for \"linear transformation\""},{"location":"03-the-architecture/03-softmax/","title":"Softmax","text":""},{"location":"03-the-architecture/03-softmax/#the-problem","title":"The Problem","text":"<p>A linear layer outputs a list of raw numbers called logits. These can be any values \u2014 positive, negative, huge, tiny:</p> <pre><code>logits = [2.1, -0.5, 1.3, 0.8, -1.2, ...]\n</code></pre> <p>But we need probabilities \u2014 numbers between 0 and 1 that sum to 1. For example: \"There's a 40% chance the next character is 'a', 30% chance it's 'e', etc.\"</p>"},{"location":"03-the-architecture/03-softmax/#the-softmax-formula","title":"The Softmax Formula","text":"\\[\\text{softmax}(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\\] <p>In plain English:</p> <ol> <li>Apply \\(e^x\\) (exponential) to each logit \u2192 makes everything positive</li> <li>Divide each by the total \u2192 makes everything sum to 1</li> </ol>"},{"location":"03-the-architecture/03-softmax/#step-by-step","title":"Step by Step","text":"1. Exponentiate2. Sum3. Divide \\[e^{2.0} = 7.389, \\quad e^{1.0} = 2.718, \\quad e^{0.1} = 1.105\\] \\[\\text{total} = 7.389 + 2.718 + 1.105 = 11.212\\] Logit \\(e^z\\) Probability 2.0 7.389 \\(7.389 / 11.212 = 0.659\\) (65.9%) 1.0 2.718 \\(2.718 / 11.212 = 0.242\\) (24.2%) 0.1 1.105 \\(1.105 / 11.212 = 0.099\\) (9.9%) Total 1.000 <p>The largest logit (2.0) gets the largest probability (65.9%). The exponential function amplifies differences.</p>"},{"location":"03-the-architecture/03-softmax/#the-code-lines-97101","title":"The Code (Lines 97\u2013101)","text":"microgpt.py \u2014 Lines 97-101<pre><code>def softmax(logits):\n    max_val = max(val.data for val in logits)\n    exps = [(val - max_val).exp() for val in logits]\n    total = sum(exps)\n    return [e / total for e in exps]\n</code></pre> <p>Wait \u2014 what's <code>max_val</code> doing there?</p> <p>Line 98 subtracts the maximum logit before exponentiating. This is the numerical stability trick.</p> <p>Without it:</p> <ul> <li>\\(e^{1000} = \\infty\\) (overflow!)</li> <li>\\(e^{-1000} = 0\\) (underflow!)</li> </ul> <p>By subtracting the max, the largest value becomes 0, and \\(e^0 = 1\\). No overflow.</p> <p>The math is unchanged \u2014 subtracting a constant from all logits doesn't change the ratios:</p> \\[\\frac{e^{a-c}}{e^{a-c} + e^{b-c}} = \\frac{e^a / e^c}{e^a / e^c + e^b / e^c} = \\frac{e^a}{e^a + e^b}\\]"},{"location":"03-the-architecture/03-softmax/#properties-of-softmax","title":"Properties of Softmax","text":"Property Why it matters All outputs are positive Probabilities can't be negative Outputs sum to 1 They represent a valid probability distribution Preserves ordering Largest logit \u2192 largest probability Differentiable We can compute gradients through it"},{"location":"03-the-architecture/03-softmax/#where-softmax-is-used","title":"Where Softmax Is Used","text":"<p>In <code>microgpt.py</code>, softmax appears in two places:</p> <ol> <li> <p>Attention weights (line 130): Converting attention scores into probabilities</p> <p>\"How much attention should I pay to each previous token?\"</p> </li> <li> <p>Output prediction (line 166): Converting final logits into character probabilities</p> <p>\"What's the probability of each possible next character?\"</p> </li> </ol> Terminology Term Meaning Logits Raw, unnormalized scores from a linear layer Softmax Function that converts logits to probabilities Probability distribution List of non-negative numbers that sum to 1 Numerical stability Avoiding overflow/underflow by shifting values \\(e^x\\) The exponential function (\\(\\approx 2.718^x\\))"},{"location":"03-the-architecture/04-normalization/","title":"Normalization (RMSNorm)","text":""},{"location":"03-the-architecture/04-normalization/#the-problem","title":"The Problem","text":"<p>As data flows through layers of linear transformations, the numbers can drift \u2014 becoming very large or very small. This causes two problems:</p> <ol> <li>Exploding values: Numbers get so large that \\(e^x\\) overflows \u2192 training crashes</li> <li>Vanishing values: Numbers get so small that they're effectively zero \u2192 model stops learning</li> </ol> <p>We need a way to keep the numbers \"well-behaved.\"</p>"},{"location":"03-the-architecture/04-normalization/#rms-normalization","title":"RMS Normalization","text":"<p><code>microgpt.py</code> uses RMSNorm (Root Mean Square Normalization), a simplified version of the more common LayerNorm.</p>"},{"location":"03-the-architecture/04-normalization/#the-formula","title":"The Formula","text":"\\[\\text{RMSNorm}(\\mathbf{x}) = \\frac{\\mathbf{x}}{\\text{RMS}(\\mathbf{x})}\\] \\[\\text{where } \\text{RMS}(\\mathbf{x}) = \\sqrt{\\frac{x_0^2 + x_1^2 + \\cdots + x_{n-1}^2}{n}}\\] <p>In words: divide each element by the \"average magnitude\" of all elements.</p>"},{"location":"03-the-architecture/04-normalization/#step-by-step","title":"Step by Step","text":"<p>Example: \\(\\mathbf{x} = [3.0, 4.0, 0.0]\\)</p> Step Computation Result 1. Square each element \\([9.0, 16.0, 0.0]\\) 2. Mean of squares \\((9.0 + 16.0 + 0.0) / 3\\) \\(8.333\\) 3. Square root (RMS) \\(\\sqrt{8.333}\\) \\(2.887\\) 4. Divide each by RMS \\([3.0/2.887, 4.0/2.887, 0.0/2.887]\\) \\([1.039, 1.386, 0.0]\\) <p>The values now have a consistent scale regardless of the original magnitudes.</p>"},{"location":"03-the-architecture/04-normalization/#the-code-lines-103106","title":"The Code (Lines 103\u2013106)","text":"microgpt.py \u2014 Lines 103-106<pre><code>def rmsnorm(x):\n    ms = sum(xi * xi for xi in x) / len(x)\n    scale = (ms + 1e-5) ** -0.5\n    return [xi * scale for xi in x]\n</code></pre> Line 104: Mean of squaresLine 105: Scale factorLine 106: Apply <pre><code>ms = sum(xi * xi for xi in x) / len(x)\n</code></pre> <p>Compute \\(x_i^2\\) for each element, sum them, divide by count. This is the \"mean square\" (MS in RMS).</p> <pre><code>scale = (ms + 1e-5) ** -0.5\n</code></pre> <p>This computes \\(\\frac{1}{\\sqrt{ms + \\epsilon}}\\):</p> <ul> <li><code>** -0.5</code> = \"1 divided by the square root\"</li> <li><code>1e-5</code> (\\(= 0.00001\\)) is a tiny epsilon (\\(\\epsilon\\)) to prevent division by zero</li> </ul> <pre><code>return [xi * scale for xi in x]\n</code></pre> <p>Multiply each element by the scale factor. Equivalent to dividing by the RMS.</p> <p>Note</p> <p>Writing <code>xi * scale</code> (where \\(\\text{scale} = 1/\\sqrt{ms}\\)) is identical to <code>xi / \u221ams</code>. Computing the reciprocal once and multiplying is slightly more efficient.</p>"},{"location":"03-the-architecture/04-normalization/#rmsnorm-vs-layernorm","title":"RMSNorm vs LayerNorm","text":"RMSNorm LayerNorm Formula \\(x / \\text{RMS}(x)\\) \\((x - \\mu) / \\sigma\\) Centers at zero? No Yes (subtracts mean) Used in LLaMA, microgpt.py GPT-2, BERT Advantage Simpler, less computation Slightly more stable <p>RMSNorm skips the mean subtraction. Research showed it works nearly as well with less computation.</p>"},{"location":"03-the-architecture/04-normalization/#where-rmsnorm-is-used","title":"Where RMSNorm Is Used","text":"<pre><code># Line 112 \u2014 after combining embeddings\nx = rmsnorm(x)\n\n# Line 117 \u2014 before attention\nx = rmsnorm(x)\n\n# Line 137 \u2014 before MLP\nx = rmsnorm(x)\n</code></pre> <p>Pre-normalization</p> <p>RMSNorm is applied before each major block (attention and MLP). This is called pre-normalization \u2014 it stabilizes the input to each block.</p> Terminology Term Meaning Normalization Scaling values to have consistent magnitude RMSNorm Dividing by the root mean square of the values LayerNorm Subtract mean, divide by standard deviation Epsilon (\\(\\epsilon\\)) A tiny number (<code>1e-5</code>) to prevent division by zero Pre-normalization Normalizing before (not after) each block"},{"location":"03-the-architecture/05-attention/","title":"Attention","text":""},{"location":"03-the-architecture/05-attention/#the-problem","title":"The Problem","text":"<p>The model has a 16-dimensional vector <code>x</code> for the current token. But language depends on context: the right next character depends on what came before. The letter after \"qua\" is likely \"r\" or \"l\" \u2014 but the model is processing one token at a time.</p> <p>How does the current token look back at previous tokens to gather context?</p>"},{"location":"03-the-architecture/05-attention/#the-intuition-library-lookup","title":"The Intuition: Library Lookup","text":"<p>The Analogy</p> <p>Imagine you're in a library (you are the current token):</p> <ul> <li>Query (Q): Your question \u2014 \"What information do I need?\"</li> <li>Key (K): Each book's title \u2014 \"Here's what I contain.\"</li> <li>Value (V): Each book's contents \u2014 \"Here's the actual information.\"</li> </ul> <p>You compare your query against each key to find relevant books, then read those books' values.</p>"},{"location":"03-the-architecture/05-attention/#the-code-setup-lines-117122","title":"The Code Setup (Lines 117\u2013122)","text":"microgpt.py \u2014 Lines 117-122<pre><code>x = rmsnorm(x)\nq = linear(x, state_dict[f'layer{li}.attn_wq'])    # query: what am I looking for?\nk = linear(x, state_dict[f'layer{li}.attn_wk'])    # key: what do I contain?\nv = linear(x, state_dict[f'layer{li}.attn_wv'])    # value: what's my information?\nkeys[li].append(k)       # store key for future tokens to see\nvalues[li].append(v)     # store value for future tokens to see\n</code></pre> <p>Each token produces:</p> <ul> <li>A query \\(\\mathbf{q}\\) (16-dim): \"What information do I need?\"</li> <li>A key \\(\\mathbf{k}\\) (16-dim): \"Here's what I contain.\"</li> <li>A value \\(\\mathbf{v}\\) (16-dim): \"Here's my actual information.\"</li> </ul> <p>All three are produced by different linear transformations of the same input <code>x</code>.</p>"},{"location":"03-the-architecture/05-attention/#scaled-dot-product-attention-lines-127131","title":"Scaled Dot-Product Attention (Lines 127\u2013131)","text":"microgpt.py \u2014 Lines 127-131 (inside the head loop)<pre><code>attn_logits = [sum(q_h[j] * k_h[t][j] for j in range(head_dim)) / head_dim**0.5\n               for t in range(len(k_h))]\nattn_weights = softmax(attn_logits)\nhead_out = [sum(attn_weights[t] * v_h[t][j] for t in range(len(v_h)))\n            for j in range(head_dim)]\n</code></pre>"},{"location":"03-the-architecture/05-attention/#step-1-compute-attention-scores","title":"Step 1: Compute Attention Scores","text":"\\[\\text{score}(q, k_t) = \\frac{\\mathbf{q} \\cdot \\mathbf{k}_t}{\\sqrt{d_k}}\\] <p>The dot product measures similarity between the current query and each past key. Divide by \\(\\sqrt{d_k}\\) (the scaling factor) to keep values from getting too large.</p>"},{"location":"03-the-architecture/05-attention/#step-2-convert-to-weights-softmax","title":"Step 2: Convert to Weights (Softmax)","text":"\\[\\alpha_t = \\text{softmax}(\\text{scores})_t\\] <p>Numerical Example</p> <p>For 3 past tokens with scores \\([2.1, 0.3, -0.5]\\):</p> Token Score Weight (\\(\\alpha\\)) Meaning \\(t_0\\) 2.1 0.73 \"Very relevant\" \\(t_1\\) 0.3 0.18 \"Somewhat relevant\" \\(t_2\\) -0.5 0.09 \"Not very relevant\""},{"location":"03-the-architecture/05-attention/#step-3-weighted-sum-of-values","title":"Step 3: Weighted Sum of Values","text":"\\[\\text{output} = \\sum_t \\alpha_t \\cdot \\mathbf{v}_t\\] <p>Blend the value vectors, weighted by how relevant each past token is.</p>"},{"location":"03-the-architecture/05-attention/#the-kv-cache","title":"The KV Cache","text":"microgpt.py \u2014 Lines 121-122<pre><code>keys[li].append(k)\nvalues[li].append(v)\n</code></pre> <p>Why cache?</p> <p>Each token's key and value are stored for future tokens to attend to. This way, when processing token 5, we have keys/values from tokens 0\u20134 already available. This avoids recomputing them \u2014 a critical optimization called the KV cache.</p>"},{"location":"03-the-architecture/05-attention/#causal-masking-built-in","title":"Causal Masking (Built-In)","text":"<p>Notice we only attend to past tokens \u2014 the ones already in the <code>keys</code> list. The current token can't see the future because future keys haven't been appended yet. This is causal masking achieved by the sequential processing order, rather than an explicit mask matrix.</p>"},{"location":"03-the-architecture/05-attention/#attention-as-a-flow","title":"Attention as a Flow","text":"<pre><code>flowchart TD\n    X[\"x (current token)\"] --&gt; Q[\"Q = linear(x, Wq)\"]\n    X --&gt; K[\"K = linear(x, Wk)\"]\n    X --&gt; V[\"V = linear(x, Wv)\"]\n    K --&gt; CACHE[\"KV Cache\\n(all past K,V)\"]\n    V --&gt; CACHE\n    Q --&gt; SCORE[\"Score = Q \u00b7 K\u1d62 / \u221ad\"]\n    CACHE --&gt; SCORE\n    SCORE --&gt; SM[\"Softmax \u2192 weights \u03b1\"]\n    SM --&gt; BLEND[\"Output = \u03a3 \u03b1\u1d62 \u00b7 V\u1d62\"]\n    CACHE --&gt; BLEND\n\n    style X fill:#1de9b6,stroke:#0db99a,color:#fff\n    style BLEND fill:#1de9b6,stroke:#0db99a,color:#000\n    style CACHE fill:#80ffe5,stroke:#5cd4bc,color:#fff</code></pre> Terminology Term Meaning Attention A mechanism where a token looks at other tokens to gather information Query (Q) \"What am I looking for?\" \u2014 generated from the current token Key (K) \"What do I contain?\" \u2014 generated from each token Value (V) \"Here's my information\" \u2014 what gets blended Attention weight How much focus to put on each past token (sums to 1) Scaled dot-product \\(\\frac{Q \\cdot K}{\\sqrt{d_k}}\\) \u2014 similarity score, scaled to prevent large values KV cache Storing past keys/values so they're not recomputed Causal masking Preventing tokens from seeing future tokens"},{"location":"03-the-architecture/06-multi-head-attention/","title":"Multi-Head Attention","text":""},{"location":"03-the-architecture/06-multi-head-attention/#the-problem","title":"The Problem","text":"<p>A single attention mechanism computes one set of attention weights \u2014 one \"perspective.\" But different aspects of language require different types of attention:</p> <ul> <li>One perspective might focus on adjacent characters (common pairs like \"th\", \"er\")</li> <li>Another might focus on the first character (names starting with certain letters tend to end certain ways)</li> <li>Another might focus on repeated patterns (\"mm\" \u2192 'a' often follows)</li> </ul> <p>One attention \"head\" can only learn one perspective. We need multiple.</p>"},{"location":"03-the-architecture/06-multi-head-attention/#the-solution-multiple-heads","title":"The Solution: Multiple Heads","text":"<p>Split the 16-dimensional Q, K, V vectors into 4 independent groups of 4 dimensions each. Each group runs its own attention. Then concatenate the results.</p> <pre><code>flowchart LR\n    Q[\"Q (16 dims)\"] --&gt; H0[\"Head 0\\ndims 0-3\"]\n    Q --&gt; H1[\"Head 1\\ndims 4-7\"]\n    Q --&gt; H2[\"Head 2\\ndims 8-11\"]\n    Q --&gt; H3[\"Head 3\\ndims 12-15\"]\n    H0 --&gt; CAT[\"Concatenate\\n(16 dims)\"]\n    H1 --&gt; CAT\n    H2 --&gt; CAT\n    H3 --&gt; CAT\n    CAT --&gt; WO[\"linear(Wo)\\n\u2192 output (16 dims)\"]\n\n    style Q fill:#1de9b6,stroke:#0db99a,color:#fff\n    style WO fill:#1de9b6,stroke:#0db99a,color:#000</code></pre> <p>Each head:</p> <ul> <li>Has its own 4-dimensional slice of Q, K, V</li> <li>Computes its own attention scores and weights</li> <li>Produces a 4-dimensional output</li> </ul>"},{"location":"03-the-architecture/06-multi-head-attention/#the-code-lines-123132","title":"The Code (Lines 123\u2013132)","text":"microgpt.py \u2014 Lines 123-132<pre><code>x_attn = []\nfor h in range(n_head):                    # n_head = 4\n    hs = h * head_dim                       # head start index: 0, 4, 8, 12\n    q_h = q[hs:hs+head_dim]                # slice of Q for this head\n    k_h = [ki[hs:hs+head_dim] for ki in keys[li]]   # slice of each K\n    v_h = [vi[hs:hs+head_dim] for vi in values[li]]  # slice of each V\n\n    # Scaled dot-product attention (per head)\n    attn_logits = [sum(q_h[j] * k_h[t][j] for j in range(head_dim)) / head_dim**0.5\n                   for t in range(len(k_h))]\n    attn_weights = softmax(attn_logits)\n    head_out = [sum(attn_weights[t] * v_h[t][j] for t in range(len(v_h)))\n                for j in range(head_dim)]\n    x_attn.extend(head_out)                 # concatenate head outputs\n</code></pre> Head 0 walkthroughAll 4 heads combined <pre><code>hs = 0 \u00d7 4 = 0\n\nq_h = q[0:4]                \u2192 first 4 dimensions of q\nk_h = [k\u2080[0:4], k\u2081[0:4], ...] \u2192 first 4 dims of each past key\nv_h = [v\u2080[0:4], v\u2081[0:4], ...] \u2192 first 4 dims of each past value\n\n\u2192 attention on dimensions 0\u20133\n\u2192 produces 4-dimensional output\n\u2192 x_attn.extend(head_out) appends these 4 values\n</code></pre> <pre><code>x_attn = [head0_out(4), head1_out(4), head2_out(4), head3_out(4)]\n       = 16 dimensions total\n</code></pre>"},{"location":"03-the-architecture/06-multi-head-attention/#output-projection-line-133","title":"Output Projection (Line 133)","text":"microgpt.py \u2014 Line 133<pre><code>x = linear(x_attn, state_dict[f'layer{li}.attn_wo'])\n</code></pre> <p>After concatenating the 4 heads, one more linear layer mixes them together. This lets the model combine information from different attention perspectives.</p>"},{"location":"03-the-architecture/06-multi-head-attention/#why-split-instead-of-4-full-size-heads","title":"Why Split Instead of 4 Full-Size Heads?","text":"Approach Operations per step 4 full-size heads (16 dims each) \\(4 \\times (16 \\times 16) = 1024\\) 4 split heads (4 dims each) \\(4 \\times (4 \\times 4) = 64\\) <p>Tip</p> <p>Same number of total parameters (the Q, K, V matrices are still 16\u00d716), but the attention computations are 16\u00d7 cheaper. And empirically, multiple small heads learn better than one big head.</p> Terminology Term Meaning Multi-head attention Running multiple attention heads in parallel on subsets of dimensions Head One independent attention mechanism head_dim The dimension of each head (\\(n_\\text{embd} / n_\\text{head}\\)) Concatenation Joining head outputs end-to-end Output projection A linear layer that mixes the concatenated heads"},{"location":"03-the-architecture/07-residual-connections/","title":"Residual Connections","text":""},{"location":"03-the-architecture/07-residual-connections/#the-problem","title":"The Problem","text":"<p>The attention block transforms <code>x</code> into something new. But what if the transformation loses important information that was in the original <code>x</code>?</p> <p>Analogy</p> <p>Imagine editing a document: you highlight a paragraph and replace it entirely. If the replacement is bad, the original is lost. But if you add a comment next to the original, you keep both.</p>"},{"location":"03-the-architecture/07-residual-connections/#the-solution-add-dont-replace","title":"The Solution: Add, Don't Replace","text":"<p>A residual connection (or \"skip connection\") is embarrassingly simple:</p> \\[\\text{output} = \\text{input} + f(\\text{input})\\] <p>Instead of replacing the input, we add the transformation on top. If the transformation learns nothing useful, it can output zeros, and the input passes through unchanged.</p>"},{"location":"03-the-architecture/07-residual-connections/#the-code-lines-116-134-136-141","title":"The Code (Lines 116, 134, 136, 141)","text":"microgpt.py \u2014 Attention block<pre><code>x_residual = x                                    # save original\n# ... attention computation (lines 117-133) ...\nx = [a + b for a, b in zip(x, x_residual)]        # add original back\n</code></pre> microgpt.py \u2014 MLP block<pre><code>x_residual = x                                    # save original\n# ... MLP computation (lines 137-140) ...\nx = [a + b for a, b in zip(x, x_residual)]        # add original back\n</code></pre> <p>Each element: \\(\\text{output}_i = \\text{transformed}_i + \\text{original}_i\\).</p>"},{"location":"03-the-architecture/07-residual-connections/#why-this-works","title":"Why This Works","text":"1. Gradient Highway2. Starting from Identity3. Preserving Information <p>During the backward pass, gradients must flow from the loss back to early parameters. Without residuals, the gradient passes through every operation and can shrink to near-zero (vanishing gradients).</p> <p>The addition creates a shortcut:</p> \\[\\frac{d(a + b)}{da} = 1 \\quad \\text{\u2190 gradient flows straight through!}\\] <p>Since the derivative of addition is 1, the gradient passes through unchanged.</p> <p>Remember that <code>attn_wo</code> and <code>mlp_fc2</code> are initialized to zero:</p> <ul> <li>Attention/MLP blocks initially output zeros</li> <li>\\(x = \\text{zeros} + x = x\\)</li> <li>The model starts as the identity function</li> </ul> <p>It then gradually learns to add useful transformations on top.</p> <p>The original information is never lost. Each block can only add new information. If a block learns nothing useful, it outputs zeros, and the input passes through unchanged.</p>"},{"location":"03-the-architecture/07-residual-connections/#visual","title":"Visual","text":"<pre><code>flowchart TD\n    X[\"x (16 dims)\"] --&gt; SAVE[\"save as x_residual\"]\n    SAVE --&gt; NORM[\"RMSNorm\"]\n    NORM --&gt; BLOCK[\"Attention or MLP\\n(transformation)\"]\n    BLOCK --&gt; ADD[\"\u2295 add\"]\n    SAVE -- \"skip connection\" --&gt; ADD\n    ADD --&gt; OUT[\"new x\\n(original + new info)\"]\n\n    style X fill:#1de9b6,stroke:#0db99a,color:#fff\n    style ADD fill:#80ffe5,stroke:#5cd4bc,color:#fff\n    style OUT fill:#1de9b6,stroke:#0db99a,color:#000</code></pre> Terminology Term Meaning Residual connection Adding the input to the output: \\(y = x + f(x)\\) Skip connection Same thing \u2014 the input \"skips over\" the transformation Vanishing gradients Gradients shrinking to near-zero in deep networks Identity function \\(f(x) = x\\) \u2014 what residuals default to"},{"location":"03-the-architecture/08-the-mlp-block/","title":"The MLP Block","text":""},{"location":"03-the-architecture/08-the-mlp-block/#the-problem","title":"The Problem","text":"<p>Attention lets tokens communicate \u2014 each token can gather information from other tokens. But attention is a linear operation (weighted sums). It can only compute linear combinations of existing information.</p> <p>To learn complex patterns (like \"after 'qu', the next letter is usually a vowel\"), the model needs non-linear processing.</p>"},{"location":"03-the-architecture/08-the-mlp-block/#the-mlp-multi-layer-perceptron","title":"The MLP (Multi-Layer Perceptron)","text":"<p>The MLP is a two-layer network sandwiched around a non-linear activation:</p> <pre><code>flowchart LR\n    X[\"x\\n(16 dims)\"] --&gt; FC1[\"Linear\\n16 \u2192 64\\n(expand)\"]\n    FC1 --&gt; ACT[\"ReLU\u00b2\\n(activate)\"]\n    ACT --&gt; FC2[\"Linear\\n64 \u2192 16\\n(compress)\"]\n    FC2 --&gt; OUT[\"output\\n(16 dims)\"]\n\n    style X fill:#1de9b6,stroke:#0db99a,color:#fff\n    style ACT fill:#64ffda,stroke:#4dd4b0,color:#fff\n    style OUT fill:#1de9b6,stroke:#0db99a,color:#000</code></pre>"},{"location":"03-the-architecture/08-the-mlp-block/#the-code-lines-135141","title":"The Code (Lines 135\u2013141)","text":"microgpt.py \u2014 Lines 135-141<pre><code># 2) MLP block\nx_residual = x                                      # save for residual\nx = rmsnorm(x)                                      # normalize\nx = linear(x, state_dict[f'layer{li}.mlp_fc1'])     # expand: 16 \u2192 64\nx = [xi.relu() ** 2 for xi in x]                    # activation: ReLU\u00b2\nx = linear(x, state_dict[f'layer{li}.mlp_fc2'])     # compress: 64 \u2192 16\nx = [a + b for a, b in zip(x, x_residual)]          # residual connection\n</code></pre> Line 138: Expand (16 \u2192 64)Line 139: Activation (ReLU\u00b2)Line 140: Compress (64 \u2192 16) <pre><code>x = linear(x, state_dict[f'layer{li}.mlp_fc1'])   # mlp_fc1 is 64 \u00d7 16\n</code></pre> <p>The first linear layer produces 64 outputs. This 4\u00d7 expansion gives the model more room to compute complex features.</p> <pre><code>x = [xi.relu() ** 2 for xi in x]\n</code></pre> <p>Applies \\(\\text{ReLU}(x)^2 = (\\max(0, x))^2\\):</p> \\[\\text{ReLU}^2(x) = \\begin{cases} x^2 &amp; \\text{if } x &gt; 0 \\\\ 0 &amp; \\text{if } x \\leq 0 \\end{cases}\\] <p>Why not just a linear function? Because two linear layers in sequence are equivalent to a single linear layer. The non-linearity is what lets the MLP compute complex functions.</p> <pre><code>x = linear(x, state_dict[f'layer{li}.mlp_fc2'])   # mlp_fc2 is 16 \u00d7 64\n</code></pre> <p>Project back down from 64 to 16 dimensions. The model \"decides\" what's worth keeping in just 16 numbers.</p>"},{"location":"03-the-architecture/08-the-mlp-block/#what-does-the-mlp-actually-do","title":"What Does the MLP Actually Do?","text":"<p>MLP as a Memory Bank</p> <p>Researchers have found that MLP layers in Transformers act as memory banks:</p> <ul> <li>The first layer's weights (64\u00d716) contain keys \u2014 patterns to match against</li> <li>The activation function acts as a gate \u2014 turning off irrelevant patterns</li> <li>The second layer's weights (16\u00d764) contain values \u2014 information to inject when a pattern matches</li> </ul> <pre><code>\"If input looks like [pattern A]\" \u2192 inject [knowledge A]\n\"If input looks like [pattern B]\" \u2192 inject [knowledge B]\n</code></pre> Terminology Term Meaning MLP Multi-Layer Perceptron \u2014 a two-layer feedforward network Activation function A non-linear function between layers (here: ReLU\u00b2) Expansion ratio Factor by which the hidden dimension expands (4\u00d7 here) Feedforward Information flows in one direction (no loops) Non-linearity Any function that isn't \\(f(x) = ax + b\\)"},{"location":"03-the-architecture/09-the-full-gpt-function/","title":"The Full GPT Function","text":""},{"location":"03-the-architecture/09-the-full-gpt-function/#putting-it-all-together","title":"Putting It All Together","text":"<p>We've studied every component. Now let's see the complete <code>gpt()</code> function \u2014 all of them assembled into a single pipeline that takes a token and produces predictions.</p>"},{"location":"03-the-architecture/09-the-full-gpt-function/#the-code-lines-108144","title":"The Code (Lines 108\u2013144)","text":"microgpt.py \u2014 Lines 108-144<pre><code>def gpt(token_id, pos_id, keys, values):\n    # Step 1: Embed\n    tok_emb = state_dict['wte'][token_id]       # token embedding lookup\n    pos_emb = state_dict['wpe'][pos_id]          # position embedding lookup\n    x = [t + p for t, p in zip(tok_emb, pos_emb)] # combine: what + where\n    x = rmsnorm(x)                                # normalize\n\n    for li in range(n_layer):                     # for each layer (just 1)\n        # 1) Multi-head attention block\n        x_residual = x\n        x = rmsnorm(x)\n        q = linear(x, state_dict[f'layer{li}.attn_wq'])\n        k = linear(x, state_dict[f'layer{li}.attn_wk'])\n        v = linear(x, state_dict[f'layer{li}.attn_wv'])\n        keys[li].append(k)\n        values[li].append(v)\n        x_attn = []\n        for h in range(n_head):\n            hs = h * head_dim\n            q_h = q[hs:hs+head_dim]\n            k_h = [ki[hs:hs+head_dim] for ki in keys[li]]\n            v_h = [vi[hs:hs+head_dim] for vi in values[li]]\n            attn_logits = [sum(q_h[j] * k_h[t][j] for j in range(head_dim)) / head_dim**0.5\n                           for t in range(len(k_h))]\n            attn_weights = softmax(attn_logits)\n            head_out = [sum(attn_weights[t] * v_h[t][j] for t in range(len(v_h)))\n                        for j in range(head_dim)]\n            x_attn.extend(head_out)\n        x = linear(x_attn, state_dict[f'layer{li}.attn_wo'])\n        x = [a + b for a, b in zip(x, x_residual)]         # residual\n\n        # 2) MLP block\n        x_residual = x\n        x = rmsnorm(x)\n        x = linear(x, state_dict[f'layer{li}.mlp_fc1'])    # expand 16\u219264\n        x = [xi.relu() ** 2 for xi in x]                    # activate\n        x = linear(x, state_dict[f'layer{li}.mlp_fc2'])    # compress 64\u219216\n        x = [a + b for a, b in zip(x, x_residual)]         # residual\n\n    logits = linear(x, state_dict['lm_head'])               # project to vocab\n    return logits\n</code></pre>"},{"location":"03-the-architecture/09-the-full-gpt-function/#the-data-flow","title":"The Data Flow","text":"Step 1: EmbeddingStep 2: Attention BlockStep 3: MLP BlockStep 4: Output <pre><code>Input: token_id = 4 ('e'), pos_id = 0\ntok_emb = wte[4]        \u2192 16 numbers representing 'e'\npos_emb = wpe[0]        \u2192 16 numbers representing \"first position\"\nx = tok_emb + pos_emb   \u2192 16 numbers: \"'e' at position 0\"\nx = rmsnorm(x)          \u2192 16 numbers, normalized\n</code></pre> <pre><code>x_residual = x                                      (save)\nx = rmsnorm(x)                                      (normalize)\nq, k, v = linear(x, Wq), linear(x, Wk), linear(x, Wv)\nFor each of 4 heads:\n  Compute attention weights over all past tokens\n  Weighted sum of values \u2192 4 numbers\nConcatenate \u2192 16 numbers\nx = linear(concat, Wo) \u2192 16 numbers\nx = x + x_residual                                  (residual)\n</code></pre> <pre><code>x_residual = x                                      (save)\nx = rmsnorm(x)                                      (normalize)\nx = linear(x, fc1)      \u2192 64 numbers (expanded)\nx = ReLU(x)\u00b2            \u2192 64 numbers (activated)\nx = linear(x, fc2)      \u2192 16 numbers (compressed)\nx = x + x_residual                                  (residual)\n</code></pre> <pre><code>logits = linear(x, lm_head) \u2192 27 numbers (one per character)\n</code></pre>"},{"location":"03-the-architecture/09-the-full-gpt-function/#the-architecture-diagram","title":"The Architecture Diagram","text":"<pre><code>flowchart TD\n    TID[\"token_id\"] --&gt; WTE[\"wte (embed)\"]\n    PID[\"pos_id\"] --&gt; WPE[\"wpe (embed)\"]\n    WTE --&gt; ADD1[\"\u2295\"]\n    WPE --&gt; ADD1\n    ADD1 --&gt; NORM0[\"RMSNorm\"]\n\n    subgraph layer[\"Layer 0\"]\n        NORM0 --&gt; SAVE1[\"save x_residual\"]\n        SAVE1 --&gt; NORM1[\"RMSNorm\"]\n        NORM1 --&gt; ATTN[\"Multi-Head Attention\\n(4 heads)\"]\n        ATTN --&gt; RES1[\"\u2295 residual\"]\n        SAVE1 -. \"skip\" .-&gt; RES1\n        RES1 --&gt; SAVE2[\"save x_residual\"]\n        SAVE2 --&gt; NORM2[\"RMSNorm\"]\n        NORM2 --&gt; MLP[\"MLP\\n(16\u219264\u219216)\"]\n        MLP --&gt; RES2[\"\u2295 residual\"]\n        SAVE2 -. \"skip\" .-&gt; RES2\n    end\n\n    RES2 --&gt; LM[\"lm_head\\n(16 \u2192 27)\"]\n    LM --&gt; OUT[\"logits\\n(27 scores)\"]\n\n    style TID fill:#1de9b6,stroke:#0db99a,color:#fff\n    style PID fill:#1de9b6,stroke:#0db99a,color:#fff\n    style OUT fill:#1de9b6,stroke:#0db99a,color:#000\n    style layer fill:none,stroke:#80ffe5,stroke-width:2px</code></pre>"},{"location":"03-the-architecture/09-the-full-gpt-function/#what-the-logits-mean","title":"What the Logits Mean","text":"<p>The output is 27 numbers \u2014 one for each token in the vocabulary:</p> <pre><code>logits[0]  \u2192 raw score for 'a'\nlogits[1]  \u2192 raw score for 'b'\n...\nlogits[25] \u2192 raw score for 'z'\nlogits[26] \u2192 raw score for &lt;BOS&gt;\n</code></pre> <p>Warning</p> <p>These are not probabilities yet. They're raw scores that can be negative or very large. To get probabilities, we apply softmax outside this function.</p>"},{"location":"03-the-architecture/09-the-full-gpt-function/#why-gpt","title":"Why \"GPT\"?","text":"<p>GPT = Generative Pre-trained Transformer</p> <ul> <li>Generative: It generates text (one token at a time)</li> <li>Pre-trained: It's trained on data before being used</li> <li>Transformer: The architecture \u2014 attention + MLP + residual connections</li> </ul> <p>Tip</p> <p>This function IS the Transformer. The rest is training and inference.</p>"},{"location":"03-the-architecture/09-the-full-gpt-function/#scaling-up","title":"Scaling Up","text":"microgpt.py GPT-2 Small GPT-3 <code>n_embd</code> 16 768 12,288 <code>n_head</code> 4 12 96 <code>n_layer</code> 1 12 96 <code>block_size</code> 8 1,024 2,048 Parameters 4,064 124M 175B <p>Same architecture. Same code. Just bigger matrices.</p> <p>Checkpoint \u2713</p> <p>You now understand the entire model architecture:</p> <ul> <li> Parameters \u2014 random numbers that encode knowledge</li> <li> Embeddings \u2014 representing tokens and positions as vectors</li> <li> Linear layers \u2014 mixing information via matrix multiplication</li> <li> Softmax \u2014 converting scores to probabilities</li> <li> RMSNorm \u2014 keeping values well-behaved</li> <li> Attention \u2014 deciding which tokens to focus on</li> <li> Multi-head \u2014 multiple attention perspectives</li> <li> Residual connections \u2014 preserving original information</li> <li> MLP \u2014 non-linear processing and knowledge storage</li> <li> Full GPT \u2014 all pieces assembled</li> </ul>"},{"location":"04-training/00-what-is-training/","title":"What Is Training?","text":""},{"location":"04-training/00-what-is-training/#the-big-idea","title":"The Big Idea","text":"<p>You have a model (<code>gpt()</code>) that takes a token and produces predictions. But right now its 4,064 parameters are random \u2014 its predictions are garbage.</p> <p>Training is the process of showing the model thousands of examples and adjusting its parameters so it gets better at predicting.</p>"},{"location":"04-training/00-what-is-training/#before-vs-after-training","title":"Before vs. After Training","text":"Before Training (random)After Training (500 steps) <pre><code>Input: 'e' at position 0\nOutput probabilities (27 characters):\n\n'a': 3.7%   'b': 3.8%   'c': 3.5%   'd': 3.9%   'e': 3.6%\n'f': 3.4%   'g': 4.0%   'h': 3.3%   'i': 4.1%   'j': 3.2%\n...\n</code></pre> <p>Nearly uniform \u2014 every character has ~3.7%. The model is guessing randomly.</p> <pre><code>Input: 'e' at position 0\nOutput probabilities:\n\n'l': 15.2%   'm': 12.8%   'r': 10.1%   'v': 9.3%   's': 8.7%\n'd': 7.2%    'a': 5.1%    'i': 4.8%    ...\n</code></pre> <p>The model learned: after 'e' in names, letters like 'l', 'm', 'r' are most common (elena, emma, erin...).</p>"},{"location":"04-training/00-what-is-training/#the-training-recipe","title":"The Training Recipe","text":"<p>Each training step follows this exact recipe:</p> <pre><code>flowchart LR\n    S[\"1. SAMPLE\\na name\"] --&gt; F[\"2. FORWARD\\npredict each\\nnext character\"]\n    F --&gt; L[\"3. LOSS\\nmeasure\\nhow wrong\"]\n    L --&gt; B[\"4. BACKWARD\\ncompute\\ngradients\"]\n    B --&gt; U[\"5. UPDATE\\nnudge\\nparameters\"]\n    U --&gt; |\"repeat \u00d7500\"| S\n\n    style S fill:#1de9b6,stroke:#0db99a,color:#fff\n    style L fill:#64ffda,stroke:#4dd4b0,color:#fff\n    style U fill:#1de9b6,stroke:#0db99a,color:#000</code></pre>"},{"location":"04-training/00-what-is-training/#the-training-loop-lines-153184-overview","title":"The Training Loop (Lines 153\u2013184, Overview)","text":"microgpt.py \u2014 Lines 152-184 (simplified)<pre><code>num_steps = 500\nfor step in range(num_steps):\n    # 1. Sample a document\n    doc = docs[step % len(docs)]\n    tokens = [BOS] + [uchars.index(ch) for ch in doc] + [BOS]\n    n = min(block_size, len(tokens) - 1)\n\n    # 2. Forward pass + 3. Loss computation\n    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n    losses = []\n    for pos_id in range(n):\n        token_id, target_id = tokens[pos_id], tokens[pos_id + 1]\n        logits = gpt(token_id, pos_id, keys, values)\n        probs = softmax(logits)\n        loss_t = -probs[target_id].log()\n        losses.append(loss_t)\n    loss = (1 / n) * sum(losses)\n\n    # 4. Backward pass\n    loss.backward()\n\n    # 5. Update parameters (Adam optimizer)\n    # ... (covered in a later lesson)\n</code></pre> <p>The flow for a name like \\\"emma\\\"</p> <ol> <li>Wrap it: <code>[BOS, 'e', 'm', 'm', 'a', BOS]</code></li> <li>For each position, predict the next character</li> <li>Compute a loss (how wrong were the predictions overall)</li> <li>Call <code>backward()</code> to compute all gradients</li> <li>Update each parameter in the direction that reduces the loss</li> </ol>"},{"location":"04-training/00-what-is-training/#how-the-loss-evolves","title":"How the Loss Evolves","text":"<pre><code>step    1 / 500 | loss 3.8912    \u2190 random predictions, high loss\nstep   50 / 500 | loss 2.8401    \u2190 starting to learn common letters\nstep  100 / 500 | loss 2.4102    \u2190 learning letter pairs\nstep  200 / 500 | loss 2.0312    \u2190 learning name-like patterns\nstep  300 / 500 | loss 1.8543    \u2190 getting good\nstep  500 / 500 | loss 1.5012    \u2190 reasonably trained\n</code></pre> <p>The loss decreasing means the model is getting better at predicting the next character.</p> <p>Why only 500 steps?</p> <p>This is a tiny model on a simple dataset. 500 steps is enough for reasonable results. Larger models on larger datasets train for millions of steps.</p> Terminology Term Meaning Training Iteratively adjusting parameters to minimize loss Training step One iteration of forward \u2192 loss \u2192 backward \u2192 update Epoch One pass through the entire dataset num_steps How many training steps to run Training loop The for-loop that runs all the training steps"},{"location":"04-training/01-the-loss-function/","title":"The Loss Function","text":""},{"location":"04-training/01-the-loss-function/#what-loss-measures","title":"What Loss Measures","text":"<p>After the model predicts probabilities for the next character, we need a single number that says: \"How wrong were you?\"</p> <p>That number is the loss. Lower = better.</p>"},{"location":"04-training/01-the-loss-function/#cross-entropy-loss","title":"Cross-Entropy Loss","text":"<p>The loss function in <code>microgpt.py</code> is cross-entropy loss:</p> \\[\\text{loss} = -\\log(P(\\text{correct token}))\\] <p>Just the negative log of the probability assigned to the correct answer.</p>"},{"location":"04-training/01-the-loss-function/#why-this-works","title":"Why This Works","text":"Confident and correctUncertainConfident and wrong \\[P(\\text{correct}) = 0.9 \\implies \\text{loss} = -\\log(0.9) = 0.105\\] <p>Low loss \u2014 the model knew the answer. </p> \\[P(\\text{correct}) = 0.2 \\implies \\text{loss} = -\\log(0.2) = 1.609\\] <p>Moderate loss \u2014 the model wasn't sure.</p> \\[P(\\text{correct}) = 0.01 \\implies \\text{loss} = -\\log(0.01) = 4.605\\] <p>High loss \u2014 the model was confident about the wrong answer. </p> <p>Warning</p> <p>Cross-entropy heavily punishes confident wrong predictions. Going from 0.01 to 0.001 adds more loss than going from 0.5 to 0.1. This forces the model not to be overconfident about wrong answers.</p>"},{"location":"04-training/01-the-loss-function/#the-loss-curve","title":"The Loss Curve","text":"\\[y = -\\log(x)\\] <pre><code>loss\n 5 \u2502 *\n   \u2502  *\n 4 \u2502   *\n   \u2502    *\n 3 \u2502     *\n   \u2502       *\n 2 \u2502         *\n   \u2502            *\n 1 \u2502                *\n   \u2502                       *\n 0 \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 *\n   0    0.2   0.4   0.6   0.8   1.0\n              P(correct)\n</code></pre> \\(P(\\text{correct})\\) Loss Interpretation 1.0 0.0 Perfect prediction 0.5 0.693 50/50 guess \\(1/27 \\approx 0.037\\) 3.296 Random chance (27 tokens) 0.01 4.605 Barely considers the correct answer"},{"location":"04-training/01-the-loss-function/#the-code-lines-166170","title":"The Code (Lines 166\u2013170)","text":"microgpt.py \u2014 Lines 166-170<pre><code>logits = gpt(token_id, pos_id, keys, values)\nprobs = softmax(logits)\nloss_t = -probs[target_id].log()\nlosses.append(loss_t)\n</code></pre> Line 166Line 167Line 168Line 169 <p><code>gpt()</code> returns 27 raw logits (scores).</p> <p><code>softmax()</code> converts logits to 27 probabilities summing to 1.</p> <p><code>probs[target_id]</code> grabs the probability of the correct next character. <code>.log()</code> computes the natural logarithm. The <code>-</code> sign makes it a positive loss.</p> <p>Append this position's loss. We'll average all positions at the end.</p>"},{"location":"04-training/01-the-loss-function/#averaging-over-the-sequence-line-171","title":"Averaging Over the Sequence (Line 171)","text":"microgpt.py \u2014 Line 171<pre><code>loss = (1 / n) * sum(losses)\n</code></pre> <p>For a name like \"emma\" (5 positions), we compute loss at each position and average:</p> \\[\\text{loss} = \\frac{1}{5}(\\text{loss}_0 + \\text{loss}_1 + \\text{loss}_2 + \\text{loss}_3 + \\text{loss}_4)\\] <p>Why average?</p> <p>Names have different lengths. Without averaging, longer names would have higher loss, biasing the model toward short names.</p>"},{"location":"04-training/01-the-loss-function/#whats-the-initial-loss","title":"What's the Initial Loss?","text":"<p>At step 0, the model assigns roughly equal probability to all 27 tokens:</p> \\[P(\\text{correct}) \\approx \\frac{1}{27} \\implies \\text{loss} \\approx -\\log\\left(\\frac{1}{27}\\right) \\approx 3.30\\] <p>If your model's first loss is near 3.3, everything is working correctly. If it's much higher, something is wrong.</p> Terminology Term Meaning Loss A single number measuring prediction error (lower = better) Cross-entropy \\(-\\log(P(\\text{correct}))\\) \u2014 the standard loss for classification Logits Raw scores before softmax Target The correct next token Average loss Mean loss over all positions in a sequence"},{"location":"04-training/02-backpropagation/","title":"Backpropagation","text":""},{"location":"04-training/02-backpropagation/#the-one-line-magic","title":"The One-Line Magic","text":"microgpt.py \u2014 Line 172<pre><code>loss.backward()\n</code></pre> <p>One line. It computes the gradient of the loss with respect to all 4,064 parameters, simultaneously.</p>"},{"location":"04-training/02-backpropagation/#what-just-happened","title":"What Just Happened?","text":"<p>If you went through Module 2, you know exactly what this does:</p> <ol> <li>Topological sort \u2014 Order all <code>Value</code> nodes so children come before parents</li> <li>Seed \u2014 Set <code>loss.grad = 1</code></li> <li>Propagate \u2014 Walk the graph in reverse:</li> </ol> \\[\\text{child.grad} \\mathrel{+}= \\text{local\\_grad} \\times \\text{parent.grad}\\] <p>The result: every <code>Value</code> in <code>params</code> now has its <code>.grad</code> field set.</p>"},{"location":"04-training/02-backpropagation/#the-scale-of-this-computation","title":"The Scale of This Computation","text":"<p>For a single training step on a 5-character name</p> <pre><code>Forward pass:  ~24,000 Value nodes created\n               (5 positions \u00d7 ~4,800 operations each)\n\nBackward pass: ~24,000 nodes visited\n               Each node: 1-2 multiplications + additions\n               Total: ~50,000 arithmetic operations\n</code></pre> <p>All of this to fill in 4,064 <code>.grad</code> values.</p>"},{"location":"04-training/02-backpropagation/#before-and-after","title":"Before and After","text":"Before <code>loss.backward()</code>After <code>loss.backward()</code> <pre><code>params[0].grad = 0       (all gradients are zero)\nparams[1].grad = 0\nparams[2].grad = 0\n...\nparams[4063].grad = 0\n</code></pre> <pre><code>params[0].grad = -0.0023   \u2190 decrease this \u2192 nudges loss down\nparams[1].grad = 0.0041    \u2190 increase this \u2192 nudges loss UP!\nparams[2].grad = -0.0001   \u2190 barely matters\n...\nparams[4063].grad = 0.0015\n</code></pre> <p>Each gradient means: \"if this parameter increased by a tiny amount, this is how much the loss would change.\"</p>"},{"location":"04-training/02-backpropagation/#why-this-is-remarkable","title":"Why This Is Remarkable","text":"<p>Without autograd, to compute 4,064 gradients, you'd need:</p> Method Forward passes per step \u00d7 500 steps Total Finite differences 4,065 \u00d7 500 ~2,000,000 Autograd 1 forward + 1 backward \u00d7 500 1,000 <p>Tip</p> <p>Autograd is ~2000\u00d7 more efficient than the brute-force approach. This is why automatic differentiation was a breakthrough.</p>"},{"location":"04-training/02-backpropagation/#the-gradient-reset-line-182","title":"The Gradient Reset (Line 182)","text":"microgpt.py \u2014 Line 182<pre><code>p.grad = 0\n</code></pre> <p>Critical</p> <p>After using each gradient for the parameter update, it's reset to zero. Why? Because <code>backward()</code> uses <code>+=</code> to accumulate gradients. Without resetting, gradients from previous steps would leak into the current step, corrupting the computation.</p> Terminology Term Meaning Backpropagation Running <code>backward()</code> on the loss to compute all gradients Gradient \\(\\partial\\text{loss}/\\partial\\text{parameter}\\) \u2014 how much the loss changes per unit parameter change Gradient reset Setting all gradients to zero before the next step"},{"location":"04-training/03-gradient-descent/","title":"Gradient Descent","text":""},{"location":"04-training/03-gradient-descent/#the-simplest-optimizer","title":"The Simplest Optimizer","text":"<p>Now that we have gradients, updating parameters is conceptually simple:</p> \\[\\theta \\leftarrow \\theta - \\eta \\cdot \\nabla\\theta\\] <p>Or in plain code:</p> <pre><code>parameter = parameter - learning_rate * gradient\n</code></pre> <p>That's gradient descent \u2014 literally \"descending\" along the gradient (slope) of the loss surface.</p>"},{"location":"04-training/03-gradient-descent/#why-subtract","title":"Why Subtract?","text":"<p>The gradient points in the direction of steepest increase of the loss. We want the loss to decrease. So we move in the opposite direction.</p> Positive gradientNegative gradient <pre><code>gradient &gt; 0\n\u2192 increasing the parameter increases the loss\n\u2192 so DECREASE the parameter (subtract)\n\nExample: param=0.5, grad=2.0, lr=0.01\n0.5 - 0.01 \u00d7 2.0 = 0.48\n</code></pre> <pre><code>gradient &lt; 0\n\u2192 increasing the parameter decreases the loss\n\u2192 so INCREASE the parameter (subtract a negative = add)\n\nExample: param=0.5, grad=-2.0, lr=0.01\n0.5 - 0.01 \u00d7 (-2.0) = 0.52\n</code></pre>"},{"location":"04-training/03-gradient-descent/#the-learning-rate","title":"The Learning Rate","text":"<p>The gradient tells us the direction to move, but not how far. The learning rate (\\(\\eta\\)) controls the step size:</p> Too largeToo smallJust right <p>Steps are so big you overshoot the minimum and the loss bounces around.</p> <p>Steps are so tiny that training takes forever.</p> <p>Steady progress toward lower loss.</p>"},{"location":"04-training/03-gradient-descent/#why-not-just-use-gradient-descent","title":"Why Not Just Use Gradient Descent?","text":"<p>Simple gradient descent (called SGD \u2014 Stochastic Gradient Descent) works but has problems:</p> Problem Why it matters One learning rate for all parameters Some parameters might need small steps, others large ones No momentum Each step only uses the current gradient. If the gradient is noisy, the path zigzags Hard to tune Learning rate is very sensitive <p>Info</p> <p>This is why <code>microgpt.py</code> uses Adam \u2014 a smarter optimizer that fixes all three problems.</p> Terminology Term Meaning Gradient descent Update: \\(\\theta \\leftarrow \\theta - \\eta \\cdot g\\) SGD Stochastic Gradient Descent \u2014 gradient descent on a random subset of data Learning rate (\\(\\eta\\)) How big each step is Convergence When the loss stops decreasing Overshoot When the learning rate is so large that updates make things worse"},{"location":"04-training/04-the-adam-optimizer/","title":"The Adam Optimizer","text":""},{"location":"04-training/04-the-adam-optimizer/#the-problem-with-plain-gradient-descent","title":"The Problem with Plain Gradient Descent","text":"<p>Gradient descent uses the same learning rate for every parameter and has no memory of past gradients. Adam fixes both.</p>"},{"location":"04-training/04-the-adam-optimizer/#what-adam-adds","title":"What Adam Adds","text":"<p>Adam (Adaptive Moment Estimation) maintains two extra values for each parameter:</p> <ol> <li>m (first moment): A running average of past gradients \u2192 momentum</li> <li>v (second moment): A running average of past squared gradients \u2192 adaptive learning rate</li> </ol>"},{"location":"04-training/04-the-adam-optimizer/#setup-lines-147149","title":"Setup (Lines 147\u2013149)","text":"microgpt.py \u2014 Lines 147-149<pre><code>learning_rate, beta1, beta2, eps_adam = 1e-2, 0.9, 0.95, 1e-8\nm = [0.0] * len(params)  # first moment buffer (momentum)\nv = [0.0] * len(params)  # second moment buffer (adaptive rates)\n</code></pre> Value What it is Typical range <code>learning_rate = 0.01</code> Base step size 1e-4 to 1e-2 <code>beta1 = 0.9</code> How much past momentum to keep 0.9 \u2013 0.99 <code>beta2 = 0.95</code> How much past variance to keep 0.95 \u2013 0.999 <code>eps_adam = 1e-8</code> Tiny number to prevent division by zero 1e-8"},{"location":"04-training/04-the-adam-optimizer/#the-update-lines-175182","title":"The Update (Lines 175\u2013182)","text":"microgpt.py \u2014 Lines 175-182<pre><code>lr_t = learning_rate * 0.5 * (1 + math.cos(math.pi * step / num_steps))\nfor i, p in enumerate(params):\n    m[i] = beta1 * m[i] + (1 - beta1) * p.grad           # update momentum\n    v[i] = beta2 * v[i] + (1 - beta2) * p.grad ** 2      # update variance\n    m_hat = m[i] / (1 - beta1 ** (step + 1))              # bias correction\n    v_hat = v[i] / (1 - beta2 ** (step + 1))              # bias correction\n    p.data -= lr_t * m_hat / (v_hat ** 0.5 + eps_adam)     # update parameter\n    p.grad = 0                                              # reset gradient\n</code></pre> Line 175: Cosine learning rate decayLine 177: Momentum (first moment)Line 178: Variance (second moment)Lines 179-180: Bias correctionLine 181: The actual update \\[\\text{lr}_t = \\text{lr} \\times \\frac{1 + \\cos(\\pi \\cdot t / T)}{2}\\] <ul> <li>Starts at <code>learning_rate</code> (0.01)</li> <li>Smoothly decreases to 0 by the end of training</li> <li>Large steps early for fast learning, tiny steps late for fine-tuning</li> </ul> <p>The formula: \\(\\cos(\\pi \\cdot t/T)\\) goes from \\(\\cos(0)=1\\) to \\(\\cos(\\pi)=-1\\). So \\(\\frac{1+\\cos(...)}{2}\\) goes from 1 \u2192 0.</p> \\[m_i = \\beta_1 \\cdot m_i + (1 - \\beta_1) \\cdot g_i = 0.9 \\cdot m_i + 0.1 \\cdot g_i\\] <p>An exponential moving average of the gradient. Keeps 90% of previous momentum, adds 10% of current gradient.</p> <p>Individual gradients are noisy (computed from a single name). Averaging smooths the noise.</p> <p>It's like a rolling ball \u2014 instead of changing direction every instant, it has inertia.</p> \\[v_i = \\beta_2 \\cdot v_i + (1 - \\beta_2) \\cdot g_i^2 = 0.95 \\cdot v_i + 0.05 \\cdot g_i^2\\] <p>Tracks how \"noisy\" each parameter's gradient is:</p> <ul> <li>Consistently large gradients \u2192 large \\(v\\) \u2192 smaller effective step</li> <li>Small gradients \u2192 small \\(v\\) \u2192 larger effective step</li> </ul> <p>This is the adaptive part \u2014 each parameter gets its own effective learning rate.</p> \\[\\hat{m} = \\frac{m_i}{1 - \\beta_1^{t+1}}, \\quad \\hat{v} = \\frac{v_i}{1 - \\beta_2^{t+1}}\\] <p>Since \\(m\\) and \\(v\\) start at 0, they're biased toward zero early on. Correction inflates them:</p> Step Correction factor for \\(m\\) Effect 0 \\(1/(1-0.9^1) = 10\\times\\) Large correction 10 \\(\\approx 2.9\\times\\) Moderate 100 \\(\\approx 1.0\\times\\) Almost none \\[\\theta_i \\leftarrow \\theta_i - \\text{lr}_t \\cdot \\frac{\\hat{m}_i}{\\sqrt{\\hat{v}_i} + \\epsilon}\\] <ul> <li>\\(\\hat{m}\\) = smoothed gradient (direction + momentum)</li> <li>\\(\\sqrt{\\hat{v}}\\) = gradient volatility (adaptive scaling)</li> <li>High variance \u2192 smaller steps (cautious)</li> <li>Low variance \u2192 larger steps (confident)</li> </ul>"},{"location":"04-training/04-the-adam-optimizer/#the-formula-in-one-line","title":"The Formula in One Line","text":"\\[\\theta \\leftarrow \\theta - \\text{lr} \\times \\frac{\\underbrace{\\text{smoothed gradient}}_{\\text{momentum}}}{\\underbrace{\\sqrt{\\text{smoothed squared gradient}}}_{\\text{adaptive rate}}}\\] Terminology Term Meaning Adam Adaptive Moment Estimation \u2014 a popular optimizer Momentum Running average of past gradients (smooths noise) Adaptive learning rate Different effective step sizes per parameter Exponential moving average \\(\\text{new} = \\beta \\cdot \\text{old} + (1-\\beta) \\cdot \\text{current}\\) Bias correction Compensating for zero-initialization in early steps Cosine decay Learning rate schedule that smoothly decreases to zero Epsilon (\\(\\epsilon\\)) Tiny constant to prevent division by zero"},{"location":"04-training/05-the-training-loop/","title":"The Training Loop","text":""},{"location":"04-training/05-the-training-loop/#the-complete-code-lines-151184","title":"The Complete Code (Lines 151\u2013184)","text":"microgpt.py \u2014 Lines 151-184<pre><code># \u2500\u2500 SETUP \u2500\u2500\nnum_steps = 500\n\nfor step in range(num_steps):\n\n    # \u2500\u2500 1. SAMPLE \u2500\u2500\n    doc = docs[step % len(docs)]        # pick a name (cycle through dataset)\n    tokens = [BOS] + [uchars.index(ch) for ch in doc] + [BOS]  # tokenize\n    n = min(block_size, len(tokens) - 1) # cap at block_size (8)\n\n    # \u2500\u2500 2. FORWARD PASS + LOSS \u2500\u2500\n    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n    losses = []\n    for pos_id in range(n):\n        token_id, target_id = tokens[pos_id], tokens[pos_id + 1]\n        logits = gpt(token_id, pos_id, keys, values)   # model prediction\n        probs = softmax(logits)                          # to probabilities\n        loss_t = -probs[target_id].log()                 # cross-entropy loss\n        losses.append(loss_t)\n    loss = (1 / n) * sum(losses)        # average loss over the sequence\n\n    # \u2500\u2500 3. BACKWARD PASS \u2500\u2500\n    loss.backward()                     # compute all gradients\n\n    # \u2500\u2500 4. PARAMETER UPDATE (Adam) \u2500\u2500\n    lr_t = learning_rate * 0.5 * (1 + math.cos(math.pi * step / num_steps))\n    for i, p in enumerate(params):\n        m[i] = beta1 * m[i] + (1 - beta1) * p.grad\n        v[i] = beta2 * v[i] + (1 - beta2) * p.grad ** 2\n        m_hat = m[i] / (1 - beta1 ** (step + 1))\n        v_hat = v[i] / (1 - beta2 ** (step + 1))\n        p.data -= lr_t * m_hat / (v_hat ** 0.5 + eps_adam)\n        p.grad = 0                      # reset gradient\n\n    # \u2500\u2500 5. LOG \u2500\u2500\n    print(f\"step {step+1:4d} / {num_steps:4d} | loss {loss.data:.4f}\")\n</code></pre>"},{"location":"04-training/05-the-training-loop/#tracing-step-0-with-emma","title":"Tracing Step 0 with \"emma\"","text":"1. Sample2. Forward + Loss3. Backward4. Update5. Log <pre><code>doc = \"emma\"\ntokens = [26, 4, 12, 12, 0, 26]     # [BOS, 'e', 'm', 'm', 'a', BOS]\nn = min(8, 6 - 1) = 5               # 5 predictions to make\n</code></pre> Position Input Target Loss 0 BOS (26) 'e' (4) 3.29 1 'e' (4) 'm' (12) 3.33 2 'm' (12) 'm' (12) 3.30 3 'm' (12) 'a' (0) 3.31 4 'a' (0) BOS (26) 3.28 Avg 3.302 <p>At step 0, the loss is ~3.3. For a random model with 27 tokens, the expected loss is \\(-\\log(1/27) \\approx 3.30\\).  The model is at random chance.</p> <pre><code>loss.backward()  # every parameter now has .grad set\n</code></pre> <pre><code>lr_t = 0.01 \u00d7 0.5 \u00d7 (1 + cos(0)) = 0.01   # full LR at step 0\n</code></pre> <p>For each of 4,064 parameters: update <code>m[i]</code>, <code>v[i]</code>, nudge <code>p.data</code>, reset <code>p.grad = 0</code>.</p> <pre><code>step    1 / 500 | loss 3.3020\n</code></pre>"},{"location":"04-training/05-the-training-loop/#the-arc-of-training","title":"The Arc of Training","text":"Step What the model has learned 1 Nothing. Random predictions. 50 Common characters predicted more often 100 Frequent letter pairs (th, er, an) 200 Name-like structures (consonant-vowel patterns) 300 When names should end (predicts BOS) 500 Reasonable name generation capability"},{"location":"04-training/05-the-training-loop/#data-cycling","title":"Data Cycling","text":"<pre><code>doc = docs[step % len(docs)]\n</code></pre> <p>The <code>%</code> (modulo) operator cycles through the dataset. With ~32,000 names and 500 steps, we only see ~1.5% of the data. A longer training run would cycle through more.</p>"},{"location":"04-training/05-the-training-loop/#block-size-truncation","title":"Block Size Truncation","text":"<pre><code>n = min(block_size, len(tokens) - 1)\n</code></pre> <p>If a name is longer than <code>block_size</code> (8), we truncate. Most names are shorter than 8 characters, so this rarely matters.</p> <p>Checkpoint \u2713</p> <p>You now understand the entire training process:</p> <ul> <li> Sampling a document and tokenizing it</li> <li> Forward pass: running the model on each token</li> <li> Loss: measuring prediction quality with cross-entropy</li> <li> Backward pass: computing all gradients automatically</li> <li> Adam optimizer: updating parameters with momentum and adaptation</li> <li> Cosine learning rate decay</li> </ul>"},{"location":"05-inference/00-generating-text/","title":"Generating Text","text":""},{"location":"05-inference/00-generating-text/#from-training-to-generation","title":"From Training to Generation","text":"<p>The model is trained. Its 4,064 parameters have been tuned over 500 steps. Now we use it to create \u2014 generating names it has never seen.</p>"},{"location":"05-inference/00-generating-text/#the-generation-loop-lines-186200","title":"The Generation Loop (Lines 186\u2013200)","text":"microgpt.py \u2014 Lines 186-200<pre><code>temperature = 0.5\n\nfor _ in range(20):                 # generate 20 names\n    tokens = []\n    token_id = BOS                   # start with BOS\n    keys = [[] for _ in range(n_layer)]\n    values = [[] for _ in range(n_layer)]\n\n    for pos_id in range(block_size): # max 8 characters\n        logits = gpt(token_id, pos_id, keys, values)\n        probs = softmax([l / temperature for l in logits])\n        token_id = random.choices(range(vocab_size), weights=[p.data for p in probs])[0]\n        if token_id == BOS:\n            break                    # BOS signals end-of-name\n        tokens.append(token_id)\n\n    print(''.join(uchars[t] for t in tokens))\n</code></pre>"},{"location":"05-inference/00-generating-text/#step-by-step","title":"Step by Step","text":"<pre><code>flowchart TD\n    START[\"Start with BOS\"] --&gt; GPT[\"gpt(token_id, pos_id)\"]\n    GPT --&gt; TEMP[\"Divide logits by temperature\"]\n    TEMP --&gt; SM[\"Softmax \u2192 probabilities\"]\n    SM --&gt; SAMPLE[\"Random sample from probabilities\"]\n    SAMPLE --&gt; CHECK{\"token_id == BOS?\"}\n    CHECK --&gt;|\"Yes\"| DONE[\"Print the name\"]\n    CHECK --&gt;|\"No\"| APPEND[\"Append character\"]\n    APPEND --&gt; GPT\n\n    style START fill:#1de9b6,stroke:#0db99a,color:#fff\n    style DONE fill:#1de9b6,stroke:#0db99a,color:#000</code></pre> Step 0Step 1Step 2Step 3Step 4Step 5 <pre><code>Input: BOS (26), position 0\nModel predicts: 'e' has highest probability\nSampled: 'e'\ntokens = ['e']\n</code></pre> <pre><code>Input: 'e' (4), position 1\nModel predicts (using KV cache from step 0): 'm' likely\nSampled: 'm'\ntokens = ['e', 'm']\n</code></pre> <pre><code>Input: 'm' (12), position 2\nModel predicts (KV cache has steps 0-1): 'm' or 'a' likely\nSampled: 'i'\ntokens = ['e', 'm', 'i']\n</code></pre> <pre><code>Input: 'i' (8), position 3\nModel predicts: 'l' likely\nSampled: 'l'\ntokens = ['e', 'm', 'i', 'l']\n</code></pre> <pre><code>Input: 'l' (11), position 4\nModel predicts: 'y' or BOS likely\nSampled: 'y'\ntokens = ['e', 'm', 'i', 'l', 'y']\n</code></pre> <pre><code>Input: 'y' (24), position 5\nModel predicts: BOS has highest probability\nSampled: BOS \u2192 STOP\nOutput: \"emily\"  (a completely new name!)\n</code></pre>"},{"location":"05-inference/00-generating-text/#key-details","title":"Key Details","text":""},{"location":"05-inference/00-generating-text/#autoregressive-generation","title":"Autoregressive Generation","text":"<p>Each output token becomes the input for the next step. The model generates one character at a time, building the name left-to-right.</p> <p>The KV cache in action</p> <p>At each step, the current token's key and value are appended to the cache. So at step 4, the model can attend to all of steps 0\u20133. No recomputation needed.</p>"},{"location":"05-inference/00-generating-text/#stopping-condition","title":"Stopping Condition","text":"<pre><code>if token_id == BOS:\n    break\n</code></pre> <p>The BOS token serves double duty \u2014 it marks both the start and end of sequences. When the model predicts BOS, it's saying \"this name is done.\"</p>"},{"location":"05-inference/00-generating-text/#no-gradient-tracking","title":"No Gradient Tracking","text":"<p>During inference, we don't call <code>backward()</code>. We're using the trained parameters as-is \u2014 just doing forward passes. This makes generation much faster than training.</p> Terminology Term Meaning Inference Using the trained model to generate predictions (no learning) Autoregressive Each output becomes the next input Sampling Randomly choosing a token based on probabilities Stopping condition When to end generation (BOS token) KV cache Stored keys/values for efficient multi-step generation"},{"location":"05-inference/01-temperature-and-sampling/","title":"Temperature and Sampling","text":""},{"location":"05-inference/01-temperature-and-sampling/#the-problem","title":"The Problem","text":"<p>When generating text, the model outputs probabilities for the next token. But how \"creative\" should the model be?</p> <ul> <li>Too predictable: Always picking the most likely token \u2192 boring, repetitive</li> <li>Too random: Picking tokens nearly uniformly \u2192 nonsensical</li> </ul> <p>The temperature parameter controls this tradeoff.</p>"},{"location":"05-inference/01-temperature-and-sampling/#what-temperature-does","title":"What Temperature Does","text":"microgpt.py \u2014 Line 195<pre><code>probs = softmax([l / temperature for l in logits])\n</code></pre> <p>Before softmax, each logit is divided by the temperature:</p> \\[\\text{adjusted logit} = \\frac{\\text{logit}}{\\text{temperature}}\\] Temperature = 1.0 (Normal)Temperature = 0.5 (Sharper)Temperature = 2.0 (Flatter) <pre><code>logits = [2.0, 1.0, 0.1]\nlogits / 1.0 = [2.0, 1.0, 0.1]\nprobs = [0.659, 0.242, 0.099]    \u2190 same as original\n</code></pre> <pre><code>logits = [2.0, 1.0, 0.1]\nlogits / 0.5 = [4.0, 2.0, 0.2]     \u2190 differences AMPLIFIED\nprobs = [0.869, 0.117, 0.014]       \u2190 the best option dominates\n</code></pre> <p>Less creative \u2014 strongly favors the top choice.</p> <pre><code>logits = [2.0, 1.0, 0.1]\nlogits / 2.0 = [1.0, 0.5, 0.05]    \u2190 differences DAMPENED\nprobs = [0.424, 0.257, 0.319]       \u2190 more uniform\n</code></pre> <p>More creative \u2014 gives weaker options a better chance.</p>"},{"location":"05-inference/01-temperature-and-sampling/#the-pattern","title":"The Pattern","text":"Temperature Effect Result \u2192 0 Probabilities become one-hot Always picks the most likely token = 1 Standard probabilities Balanced \u2192 \u221e Probabilities become uniform Pure random <p>Why temperature = 0.5 in microgpt.py?</p> <p>A temperature of 0.5 makes the model fairly confident \u2014 it strongly favors high-probability characters. This produces more \"realistic\" names. Higher temperatures produce more creative but potentially nonsensical combinations.</p>"},{"location":"05-inference/01-temperature-and-sampling/#the-sampling-step","title":"The Sampling Step","text":"microgpt.py \u2014 Line 196<pre><code>token_id = random.choices(range(vocab_size), weights=[p.data for p in probs])[0]\n</code></pre> <p><code>random.choices(population, weights)</code> selects one item where each item's chance is proportional to its weight.</p> <p>Even with temperature = 0.5, there's still randomness. The most likely character is usually picked, but not always. This is what makes each generation unique.</p>"},{"location":"05-inference/01-temperature-and-sampling/#alternative-sampling-methods","title":"Alternative Sampling Methods","text":"Method How it works Tradeoff Greedy Always pick highest probability Deterministic, repetitive Random Pick based on probabilities (what we do) Varied, sometimes odd Top-k Only consider the top k most likely tokens Less randomness Nucleus (top-p) Consider tokens until cumulative prob reaches p Adaptive k <p><code>microgpt.py</code> uses simple random sampling with temperature \u2014 the most straightforward approach.</p> <p>Why is it called 'temperature'?</p> <p>From statistical mechanics in physics:</p> <ul> <li>High temperature \u2192 particles move randomly (high entropy)</li> <li>Low temperature \u2192 particles settle into ordered states (low entropy)</li> </ul> <p>Same idea: high temperature = more randomness, low temperature = more order.</p> Terminology Term Meaning Temperature A scalar that controls randomness in generation Sharpening Low temperature makes the distribution more peaked Flattening High temperature makes the distribution more uniform Greedy decoding Always choosing the most likely token Entropy A measure of randomness/uncertainty in a distribution"},{"location":"05-inference/02-the-complete-picture/","title":"The Complete Picture","text":""},{"location":"05-inference/02-the-complete-picture/#youve-made-it","title":"You've Made It","text":"<p>If you've read through every lesson in order, you now understand every single line of <code>microgpt.py</code>. Let's zoom out and see the whole thing as one coherent story.</p>"},{"location":"05-inference/02-the-complete-picture/#the-story-of-200-lines","title":"The Story of 200 Lines","text":"Act 1: The World (Lines 1\u201327)Act 2: The Engine (Lines 29\u201372)Act 3: The Mind (Lines 74\u2013144)Act 4: The Training (Lines 146\u2013184)Act 5: The Voice (Lines 186\u2013200) <p>\"Let there be data.\"</p> <p>We start with a text file of 32,000 human names. Each name is a sequence of characters. We build a tokenizer: 26 letters + 1 BOS token = 27 tokens total.</p> <p>What we built: A dataset and a way to encode/decode text.</p> <p>\"Let there be learning.\"</p> <p>We build a tiny automatic differentiation engine \u2014 the <code>Value</code> class. Every number remembers how it was computed. When we call <code>backward()</code>, it walks the computation graph in reverse, computing derivatives using the chain rule.</p> <p>What we built: An autograd engine that makes training possible.</p> <p>\"Let there be intelligence.\"</p> <p>We initialize ~4,000 random parameters and define the architecture:</p> <p>Embed \u2192 Normalize \u2192 Attend \u2192 Think \u2192 Predict</p> <ul> <li>Embeddings give each token a rich representation</li> <li>Attention lets tokens look at their context</li> <li>MLP does non-linear processing</li> <li>Residual connections preserve information</li> <li>RMSNorm keeps values stable</li> </ul> <p>What we built: A Transformer that maps tokens to predictions.</p> <p>\"Let there be knowledge.\"</p> <p>The model starts knowing nothing. Over 500 steps, we show it names, measure prediction error (cross-entropy), compute gradients (backpropagation), and adjust parameters (Adam with cosine decay).</p> <p>The loss drops from ~3.3 (random chance) to ~1.5 (reasonably good).</p> <p>What we built: A training loop that instills knowledge into parameters.</p> <p>\"Let there be creation.\"</p> <p>We generate 20 new names: start with BOS, predict characters one at a time, sample with temperature=0.5, stop when BOS reappears.</p> <p>What we built: An inference loop that generates new text.</p>"},{"location":"05-inference/02-the-complete-picture/#the-complete-dependency-map","title":"The Complete Dependency Map","text":"<pre><code>flowchart TD\n    DATA[\"Data\\n(names.txt)\"] --&gt; TOK[\"Tokenizer\\n(chars \u2192 IDs)\"]\n    TOK --&gt; AG[\"Autograd Engine\\n(Value class)\"]\n    AG --&gt; PARAMS[\"Parameters\\n(4,064 Values)\"]\n    AG --&gt; ARCH[\"Architecture\\n(gpt function)\"]\n    PARAMS --&gt; TRAIN[\"Training Loop\"]\n    ARCH --&gt; TRAIN\n    TRAIN --&gt; TRAINED[\"Trained Parameters\"]\n    TRAINED --&gt; INF[\"Inference Loop\"]\n    INF --&gt; NAMES[\"Generated Names\"]\n\n    style DATA fill:#1de9b6,stroke:#0db99a,color:#fff\n    style TRAINED fill:#80ffe5,stroke:#5cd4bc,color:#fff\n    style NAMES fill:#1de9b6,stroke:#0db99a,color:#000</code></pre>"},{"location":"05-inference/02-the-complete-picture/#microgptpy-vs-chatgpt","title":"microgpt.py vs. ChatGPT","text":"microgpt.py ChatGPT Same algorithm? Character-level tokenizer BPE tokenizer (50k+ tokens) <code>Value</code> class (Python) PyTorch autograd (CUDA) 4,064 params 175B+ params 1 layer, 4 heads 96 layers, 96 heads 500 training steps Millions of steps 1 CPU Thousands of GPUs names.txt Terabytes of internet text <p>Everything else is just efficiency</p> <p>The algorithm is identical. The differences are scale (more parameters, data, compute), speed (GPU acceleration), and polish (better tokenizers, fine-tuning, RLHF). But the fundamental loop \u2014 embed, attend, predict, compute loss, backpropagate, update \u2014 is the same.</p>"},{"location":"05-inference/02-the-complete-picture/#concepts-you-now-understand","title":"Concepts You Now Understand","text":"Concept What you know Tokenization Converting text to numbers and back Embeddings Representing tokens as learnable vectors Attention \\(Q \\cdot K / \\sqrt{d_k}\\) to compute relevance, weighted sum of \\(V\\) Multi-head attention Multiple parallel attention perspectives Residual connections Skip connections that preserve information RMSNorm Keeping values well-scaled MLP Non-linear processing (expand \u2192 activate \u2192 compress) Forward pass Computing predictions and building the graph Backward pass Computing gradients via chain rule Cross-entropy loss \\(-\\log(P(\\text{correct}))\\) Adam optimizer Momentum + adaptive learning rates Temperature Controlling generation randomness Autoregressive generation Each output becomes the next input"},{"location":"05-inference/02-the-complete-picture/#where-to-go-from-here","title":"Where to Go From Here","text":"<p>Experiments to try</p> <ul> <li>Change <code>n_embd</code> (16 \u2192 32) and see the effect</li> <li>Change <code>temperature</code> (0.5 \u2192 0.1, 1.0, 2.0)</li> <li>Train for more steps (500 \u2192 2000)</li> <li>Use a different dataset (cities, words, anything)</li> </ul> <p>Further reading</p> <ul> <li>Karpathy's \"Let's build GPT\" \u2014 video version of this journey</li> <li>The original Transformer paper: \"Attention Is All You Need\"</li> <li>GPT-2 paper</li> </ul>"},{"location":"05-inference/02-the-complete-picture/#the-final-analogy","title":"The Final Analogy","text":"<p>It's like learning that a car engine has just four strokes: intake, compress, ignite, exhaust. Everything else \u2014 turbochargers, fuel injection, cooling systems \u2014 is optimization. But the four strokes ARE the engine.</p> <p>In our case:</p> <ul> <li>Embed (intake)</li> <li>Attend + Transform (compress + ignite)</li> <li>Predict \u2192 Loss \u2192 Gradient \u2192 Update (exhaust + repeat)</li> </ul> <p>That's the engine. You now understand every moving part.</p>"},{"location":"appendix/glossary/","title":"Glossary","text":"<p>A reference of every term used in the course, in alphabetical order.</p> A <p>Activation Function \u2014 A non-linear function applied between linear layers. Enables networks to learn complex patterns. In microgpt.py: ReLU\u00b2 (\\(\\max(0, x)^2\\)).</p> <p>Adam \u2014 Adaptive Moment Estimation. An optimizer that combines momentum (running average of gradients) and adaptive learning rates (per-parameter step sizes). Lines 174\u2013182.</p> <p>Attention \u2014 A mechanism that lets tokens \"look at\" other tokens to gather context. Computes relevance scores via dot products between queries and keys, then takes a weighted average of values.</p> <p>Autograd \u2014 Automatic differentiation. A system that computes derivatives automatically by recording operations and replaying them in reverse. The <code>Value</code> class implements this.</p> <p>Autoregressive \u2014 A generation strategy where each output token becomes the input for producing the next token.</p> B <p>Backward Pass \u2014 Walking the computation graph in reverse to compute gradients using the chain rule. Triggered by <code>loss.backward()</code>.</p> <p>Bias Correction \u2014 A fix for the zero-initialization of Adam's moment estimates, important in early training steps.</p> <p>Block Size \u2014 The maximum sequence length the model can process. Set to 8 in microgpt.py.</p> <p>BOS (Beginning of Sequence) \u2014 A special token (ID 26) used to mark the start and end of a sequence.</p> C\u2013D <p>Causal Masking \u2014 Preventing the model from attending to future tokens. In microgpt.py, this happens naturally because tokens are processed one at a time.</p> <p>Chain Rule \u2014 The derivative of a composition of functions equals the product of the individual derivatives: \\(\\frac{d}{dx}f(g(x)) = f'(g(x)) \\cdot g'(x)\\).</p> <p>Computation Graph \u2014 A tree of <code>Value</code> nodes recording all operations performed during the forward pass.</p> <p>Cosine Decay \u2014 A learning rate schedule that smoothly decreases from the initial value to zero using a cosine curve.</p> <p>Cross-Entropy Loss \u2014 The loss function \\(-\\log(P(\\text{correct token}))\\). Heavily penalizes confident wrong predictions.</p> <p>Dataset \u2014 The collection of training examples. Here: ~32,000 human names from <code>input.txt</code>.</p> <p>Derivative \u2014 The rate of change of a function's output with respect to its input. Tells us \"which direction to nudge.\"</p> <p>Document \u2014 A single training example. In this case, one name (e.g., \"emma\").</p> <p>Dot Product \u2014 Multiply corresponding elements and sum: \\(\\text{dot}(\\mathbf{a}, \\mathbf{b}) = a_0 b_0 + a_1 b_1 + \\cdots\\). Measures similarity.</p> E\u2013G <p>Embedding \u2014 A learnable vector representation of a token. Converts token IDs into rich numerical representations.</p> <p>Embedding Table \u2014 A matrix where row \\(i\\) is the embedding for token \\(i\\).</p> <p>Epoch \u2014 One complete pass through the entire dataset.</p> <p>Epsilon (\\(\\epsilon\\)) \u2014 A tiny number (e.g., \\(10^{-5}\\) or \\(10^{-8}\\)) added to prevent division by zero.</p> <p>Exponential Moving Average \u2014 \\(\\text{new} = \\beta \\cdot \\text{old} + (1-\\beta) \\cdot \\text{current}\\). Smooths a sequence of values.</p> <p>Forward Pass \u2014 Computing the output from the input, step by step, creating the computation graph.</p> <p>Gradient \u2014 The derivative of the loss with respect to a parameter. Tells us how to adjust the parameter to reduce the loss.</p> <p>Gradient Accumulation \u2014 Summing gradient contributions from multiple paths through the graph (\\(+=\\) in backward).</p> <p>Gradient Descent \u2014 The simplest optimizer: \\(\\theta \\leftarrow \\theta - \\eta \\cdot g\\).</p> H\u2013L <p>Head (Attention Head) \u2014 One independent attention mechanism operating on a subset of dimensions.</p> <p>Hyperparameter \u2014 A setting chosen by the programmer (<code>n_embd</code>, <code>n_head</code>, <code>learning_rate</code>, etc.), not learned during training.</p> <p>Inference \u2014 Using the trained model to generate new predictions, without updating parameters.</p> <p>KV Cache \u2014 Storing keys and values from previous tokens to avoid recomputation during generation.</p> <p>Layer \u2014 One complete attention+MLP block in the Transformer (microgpt.py has 1 layer).</p> <p>Learning Rate \u2014 Step size for parameter updates. Controls how much parameters change each step.</p> <p>Linear Layer \u2014 Matrix multiplication: \\(y = Wx\\). Mixes and recombines information.</p> <p>Local Gradient \u2014 The derivative of a single operation with respect to its immediate input.</p> <p>Logits \u2014 Raw, unnormalized scores output by the model before softmax.</p> <p>Loss \u2014 A single number measuring how wrong the model's predictions were. Lower is better.</p> M\u2013P <p>MLP (Multi-Layer Perceptron) \u2014 A two-layer feedforward network with a non-linear activation in between. Expand \u2192 Activate \u2192 Compress.</p> <p>Momentum \u2014 A running average of past gradients, used in Adam to smooth out noisy updates.</p> <p>Multi-Head Attention \u2014 Running multiple attention heads in parallel, each on a subset of dimensions, then concatenating results.</p> <p>Normalization \u2014 Scaling values to have consistent magnitude. Prevents numerical instability.</p> <p>Output Projection \u2014 A linear layer applied after multi-head attention to mix the concatenated head outputs.</p> <p>Parameters \u2014 The learnable numbers in the model. Start random, get tuned during training.</p> <p>Position Embedding \u2014 A vector encoding a token's position in the sequence.</p> <p>Pre-normalization \u2014 Applying normalization before (not after) each block. Used in microgpt.py.</p> <p>Probability Distribution \u2014 A list of non-negative numbers that sum to 1.</p> Q\u2013S <p>Query (Q) \u2014 In attention: \"What am I looking for?\" The current token's search vector.</p> <p>Key (K) \u2014 In attention: \"What do I offer?\" Each token's advertisement vector.</p> <p>Value (V) \u2014 In attention: \"Here's my content.\" The actual information a token provides.</p> <p>ReLU \u2014 Rectified Linear Unit: \\(\\max(0, x)\\). A simple activation function.</p> <p>Residual Connection \u2014 Adding the input back to the output: \\(y = x + f(x)\\).</p> <p>RMSNorm \u2014 Root Mean Square Normalization: \\(x / \\sqrt{\\text{mean}(x^2)}\\).</p> <p>Sampling \u2014 Randomly choosing the next token based on the probability distribution.</p> <p>Scaled Attention \u2014 Dividing attention scores by \\(\\sqrt{d_k}\\) to prevent softmax saturation.</p> <p>Sequence \u2014 An ordered list of tokens.</p> <p>Softmax \u2014 Function that converts logits to probabilities: \\(e^{x_i} / \\sum_j e^{x_j}\\).</p> <p>State Dict \u2014 A dictionary mapping parameter names to weight matrices.</p> T\u2013Z <p>Temperature \u2014 A scalar that controls randomness during generation. Low = deterministic, high = creative.</p> <p>Token \u2014 The smallest unit the model processes. In microgpt.py: individual characters.</p> <p>Tokenizer \u2014 The system that converts between text and token IDs.</p> <p>Topological Sort \u2014 Ordering graph nodes so children always come before parents. Needed for backward pass.</p> <p>Training \u2014 The process of iteratively adjusting parameters to minimize loss.</p> <p>Training Step \u2014 One complete forward \u2192 loss \u2192 backward \u2192 update cycle.</p> <p>Transformer \u2014 The architecture: attention + MLP + residual connections + normalization.</p> <p>Vocabulary \u2014 The complete set of all possible tokens (27 in microgpt.py).</p> <p>Vocab Size \u2014 The number of unique tokens (27 = 26 letters + BOS).</p> <p>Weight Matrix \u2014 A 2D grid of learnable parameters used in linear transformations.</p>"},{"location":"appendix/math-refresher/","title":"Math Refresher","text":"<p>Everything you need to know for this course \u2014 and nothing more.</p>"},{"location":"appendix/math-refresher/#1-exponents","title":"1. Exponents","text":"<p>What: Repeated multiplication.</p> \\[2^3 = 2 \\times 2 \\times 2 = 8, \\quad 5^2 = 5 \\times 5 = 25\\] <p>Rules:</p> Rule Example \\(a^m \\times a^n = a^{m+n}\\) \\(2^3 \\times 2^2 = 2^5 = 32\\) \\(a^{-n} = 1 / a^n\\) \\(2^{-3} = 1/8\\) \\(a^{1/2} = \\sqrt{a}\\) \\(9^{1/2} = 3\\) \\(a^{-1/2} = 1/\\sqrt{a}\\) \\(4^{-0.5} = 1/2\\) \\(a^0 = 1\\) \\(5^0 = 1\\) <p>Where it appears in microgpt.py</p> <ul> <li><code>(ms + 1e-5) ** -0.5</code> \u2014 computing \\(1/\\sqrt{\\text{mean square}}\\) in RMSNorm</li> <li><code>self.data**other</code> \u2014 the power operation in the <code>Value</code> class</li> </ul>"},{"location":"appendix/math-refresher/#2-the-exponential-function-ex","title":"2. The Exponential Function (\\(e^x\\))","text":"<p>The number \\(e \\approx 2.718\\) raised to the power \\(x\\):</p> \\[e^0 = 1, \\quad e^1 \\approx 2.718, \\quad e^2 \\approx 7.389, \\quad e^{-1} \\approx 0.368\\] <p>Key properties:</p> <ul> <li>Always positive: \\(e^x &gt; 0\\) for all \\(x\\)</li> <li>Grows very fast for large \\(x\\)</li> <li>Approaches zero for large negative \\(x\\)</li> <li>Its derivative is itself: \\(\\frac{d}{dx}e^x = e^x\\)</li> </ul> <p>Where it appears</p> <p><code>math.exp(self.data)</code> \u2014 used in softmax to make all values positive.</p>"},{"location":"appendix/math-refresher/#3-the-logarithm-ln-or-log","title":"3. The Logarithm (\\(\\ln\\) or \\(\\log\\))","text":"<p>The inverse of the exponential. \\(\\ln(x)\\) answers: \"what power do I raise \\(e\\) to, to get \\(x\\)?\"</p> \\[\\ln(1) = 0, \\quad \\ln(e) = 1, \\quad \\ln(7.389) \\approx 2\\] <p>Key properties:</p> <ul> <li>Only defined for positive numbers</li> <li>\\(\\ln(1) = 0\\)</li> <li>\\(\\ln(x) &lt; 0\\) when \\(0 &lt; x &lt; 1\\)</li> <li>Derivative: \\(\\frac{d}{dx}\\ln(x) = 1/x\\)</li> </ul> <p>Where it appears</p> <p><code>-probs[target_id].log()</code> \u2014 the cross-entropy loss function.</p>"},{"location":"appendix/math-refresher/#4-summation-sigma","title":"4. Summation (\\(\\Sigma\\))","text":"<p>Shorthand for \"add up a bunch of things\":</p> \\[\\sum_{i=1}^{3} x_i = x_1 + x_2 + x_3\\] <p>In Python: <code>sum(x[i] for i in range(3))</code></p>"},{"location":"appendix/math-refresher/#5-derivatives-basics","title":"5. Derivatives (Basics)","text":"<p>The derivative \\(\\frac{dy}{dx}\\) tells you: \"if \\(x\\) changes by a tiny bit, how much does \\(y\\) change?\"</p> Function Derivative In English \\(y = c\\) \\(\\frac{dy}{dx} = 0\\) Constants don't change \\(y = x\\) \\(\\frac{dy}{dx} = 1\\) 1-to-1 relationship \\(y = cx\\) \\(\\frac{dy}{dx} = c\\) Scales the change \\(y = x^2\\) \\(\\frac{dy}{dx} = 2x\\) \\(y = x^n\\) \\(\\frac{dy}{dx} = nx^{n-1}\\) Power rule \\(y = e^x\\) \\(\\frac{dy}{dx} = e^x\\) Its own derivative \\(y = \\ln(x)\\) \\(\\frac{dy}{dx} = 1/x\\) \\(y = \\max(0,x)\\) \\(\\frac{dy}{dx} = \\begin{cases}1 &amp; x&gt;0 \\\\ 0 &amp; x \\leq 0\\end{cases}\\) Step function <p>The Chain Rule:</p> \\[\\frac{d}{dx}f(g(x)) = f'(g(x)) \\cdot g'(x)\\] <p>\"Multiply the derivatives along the chain.\"</p>"},{"location":"appendix/math-refresher/#6-vectors-lists-of-numbers","title":"6. Vectors (Lists of Numbers)","text":"<p>A vector is a list of numbers:</p> <pre><code>v = [3.0, -1.0, 2.5]    # a 3-dimensional vector\n</code></pre> Operation Example Result Addition \\([1, 2] + [3, 4]\\) \\([4, 6]\\) Scalar multiplication \\(2 \\times [3, 4]\\) \\([6, 8]\\) Dot product \\([1, 2] \\cdot [3, 4]\\) \\(1 \\times 3 + 2 \\times 4 = 11\\) <p>Every embedding, every layer input/output is a vector.</p>"},{"location":"appendix/math-refresher/#7-matrices-grids-of-numbers","title":"7. Matrices (Grids of Numbers)","text":"<p>A matrix is a 2D grid:</p> <pre><code>M = [[1, 2, 3],\n     [4, 5, 6]]    # a 2\u00d73 matrix\n</code></pre> <p>Matrix-vector multiplication \u2014 the <code>linear()</code> function in microgpt.py:</p> \\[\\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 14 \\\\ 32 \\end{bmatrix}\\] <p>Each row's dot product with the input gives one output element.</p>"},{"location":"appendix/math-refresher/#8-probability","title":"8. Probability","text":"<p>A probability distribution assigns a number between 0 and 1 to each outcome, with all probabilities summing to 1:</p> \\[P(\\text{a}) = 0.3, \\quad P(\\text{b}) = 0.5, \\quad P(\\text{c}) = 0.2 \\quad \\implies \\quad \\text{Sum} = 1.0\\] <p>Random sampling: Choosing an outcome where each option's chance equals its probability.</p>"},{"location":"appendix/math-refresher/#9-square-root-sqrtphantomx","title":"9. Square Root (\\(\\sqrt{\\phantom{x}}\\))","text":"\\[\\sqrt{4} = 2, \\quad \\sqrt{9} = 3, \\quad \\sqrt{2} \\approx 1.414\\] <p>In code: <code>x ** 0.5</code> or <code>math.sqrt(x)</code></p> <p>Where it appears</p> <ul> <li><code>head_dim**0.5</code> \u2014 scaling in attention</li> <li><code>v_hat ** 0.5</code> \u2014 in Adam optimizer</li> </ul>"},{"location":"appendix/math-refresher/#10-mean-average","title":"10. Mean (Average)","text":"\\[\\text{mean}([2, 4, 6]) = \\frac{2 + 4 + 6}{3} = 4\\] <p>In code: <code>sum(x) / len(x)</code></p> <p>Where it appears</p> <p><code>sum(xi * xi for xi in x) / len(x)</code> \u2014 mean of squares in RMSNorm.</p> <p>That's all the math. Every formula in microgpt.py uses only these concepts.</p>"}]}