# Course Index

## Module 0: The Big Picture

| # | Lesson | Lines |
|---|--------|-------|
| 0.0 | [What Is a Language Model?](./00-the-big-picture/00-what-is-a-language-model.md) | — |
| 0.1 | [The 200-Line Map](./00-the-big-picture/01-the-200-line-map.md) | 1–200 |
| 0.2 | [The Learning Machine Analogy](./00-the-big-picture/02-the-learning-machine-analogy.md) | — |

---

## Module 1: Data & Tokenization

| # | Lesson | Lines |
|---|--------|-------|
| 1.0 | [The Dataset](./01-data-and-tokenization/00-the-dataset.md) | 14–21 |
| 1.1 | [Characters as Numbers](./01-data-and-tokenization/01-characters-as-numbers.md) | 23–27 |
| 1.2 | [The BOS Token](./01-data-and-tokenization/02-the-bos-token.md) | 25–26 |

---

## Module 2: Calculus & Autograd

| # | Lesson | Lines |
|---|--------|-------|
| 2.0 | [Why We Need Derivatives](./02-calculus-and-autograd/00-why-we-need-derivatives.md) | — |
| 2.1 | [The Chain Rule](./02-calculus-and-autograd/01-the-chain-rule.md) | — |
| 2.2 | [The Value Class](./02-calculus-and-autograd/02-the-value-class.md) | 30–57 |
| 2.3 | [Forward Pass](./02-calculus-and-autograd/03-forward-pass.md) | — |
| 2.4 | [Backward Pass](./02-calculus-and-autograd/04-backward-pass.md) | 59–72 |
| 2.5 | [Building a Computation Graph](./02-calculus-and-autograd/05-building-a-computation-graph.md) | — |

---

## Module 3: The Architecture

| # | Lesson | Lines |
|---|--------|-------|
| 3.0 | [Parameters Are Knowledge](./03-the-architecture/00-parameters-are-knowledge.md) | 74–90 |
| 3.1 | [Embeddings](./03-the-architecture/01-embeddings.md) | 109–111 |
| 3.2 | [Linear Layers](./03-the-architecture/02-linear-layers.md) | 94–95 |
| 3.3 | [Softmax](./03-the-architecture/03-softmax.md) | 97–101 |
| 3.4 | [Normalization (RMSNorm)](./03-the-architecture/04-normalization.md) | 103–106 |
| 3.5 | [Attention](./03-the-architecture/05-attention.md) | 118–133 |
| 3.6 | [Multi-Head Attention](./03-the-architecture/06-multi-head-attention.md) | 123–133 |
| 3.7 | [Residual Connections](./03-the-architecture/07-residual-connections.md) | 116, 134, 136, 141 |
| 3.8 | [The MLP Block](./03-the-architecture/08-the-mlp-block.md) | 135–141 |
| 3.9 | [The Full GPT Function](./03-the-architecture/09-the-full-gpt-function.md) | 108–144 |

---

## Module 4: Training

| # | Lesson | Lines |
|---|--------|-------|
| 4.0 | [What Is Training?](./04-training/00-what-is-training.md) | 151–184 |
| 4.1 | [The Loss Function](./04-training/01-the-loss-function.md) | 160–169 |
| 4.2 | [Backpropagation](./04-training/02-backpropagation.md) | 172 |
| 4.3 | [Gradient Descent](./04-training/03-gradient-descent.md) | — |
| 4.4 | [The Adam Optimizer](./04-training/04-the-adam-optimizer.md) | 146–182 |
| 4.5 | [The Training Loop](./04-training/05-the-training-loop.md) | 151–184 |

---

## Module 5: Inference & Generation

| # | Lesson | Lines |
|---|--------|-------|
| 5.0 | [Generating Text](./05-inference/00-generating-text.md) | 186–200 |
| 5.1 | [Temperature and Sampling](./05-inference/01-temperature-and-sampling.md) | 187, 195 |
| 5.2 | [The Complete Picture](./05-inference/02-the-complete-picture.md) | 1–200 |

---

## Appendix

| # | Reference | Description |
|---|-----------|-------------|
| A.1 | [Glossary](./appendix/glossary.md) | 60+ terms defined |
| A.2 | [Math Refresher](./appendix/math-refresher.md) | Exponents, log, derivatives, vectors, matrices |
